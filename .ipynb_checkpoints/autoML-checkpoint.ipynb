{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier, RidgeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "from find_accuracy import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lese fil og fjerne indeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>texture_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>smoothness_se</th>\n",
       "      <th>compactness_se</th>\n",
       "      <th>concavity_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  ...  symmetry_worst  fractal_dimension_worst  Unnamed: 32\n",
       "0    842302         M        17.99  ...          0.4601                  0.11890          NaN\n",
       "1    842517         M        20.57  ...          0.2750                  0.08902          NaN\n",
       "2  84300903         M        19.69  ...          0.3613                  0.08758          NaN\n",
       "3  84348301         M        11.42  ...          0.6638                  0.17300          NaN\n",
       "4  84358402         M        20.29  ...          0.2364                  0.07678          NaN\n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"id\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>texture_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>smoothness_se</th>\n",
       "      <th>compactness_se</th>\n",
       "      <th>concavity_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  diagnosis  radius_mean  texture_mean  ...  concave points_worst  symmetry_worst  fractal_dimension_worst\n",
       "0         M        17.99         10.38  ...                0.2654          0.4601                  0.11890\n",
       "1         M        20.57         17.77  ...                0.1860          0.2750                  0.08902\n",
       "2         M        19.69         21.25  ...                0.2430          0.3613                  0.08758\n",
       "3         M        11.42         20.38  ...                0.2575          0.6638                  0.17300\n",
       "4         M        20.29         14.34  ...                0.1625          0.2364                  0.07678\n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"diagnosis\"] = data[\"diagnosis\"].replace({'M': 1, 'B': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>texture_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>smoothness_se</th>\n",
       "      <th>compactness_se</th>\n",
       "      <th>concavity_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diagnosis  radius_mean  texture_mean  ...  concave points_worst  symmetry_worst  fractal_dimension_worst\n",
       "0          1        17.99         10.38  ...                0.2654          0.4601                  0.11890\n",
       "1          1        20.57         17.77  ...                0.1860          0.2750                  0.08902\n",
       "2          1        19.69         21.25  ...                0.2430          0.3613                  0.08758\n",
       "3          1        11.42         20.38  ...                0.2575          0.6638                  0.17300\n",
       "4          1        20.29         14.34  ...                0.1625          0.2364                  0.07678\n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    357\n",
       "1    212\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"diagnosis\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fjerne outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.abs(stats.zscore(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   3,   3,   3,   3,   3,   3,   3,   3,   9,   9,   9,  12,\n",
       "         12,  12,  12,  12,  14,  14,  23,  25,  31,  31,  35,  42,  42,\n",
       "         42,  60,  68,  68,  68,  68,  71,  71,  71,  71,  72,  78,  78,\n",
       "         78,  78,  78,  82,  82,  82,  82,  82,  82,  82,  83, 105, 105,\n",
       "        108, 108, 108, 108, 108, 108, 112, 112, 116, 119, 119, 122, 122,\n",
       "        122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122,\n",
       "        138, 138, 146, 146, 146, 151, 151, 152, 152, 152, 152, 152, 152,\n",
       "        176, 176, 180, 180, 180, 180, 180, 180, 180, 181, 181, 190, 190,\n",
       "        190, 190, 190, 192, 202, 203, 212, 212, 212, 212, 212, 212, 212,\n",
       "        213, 213, 213, 213, 213, 219, 219, 232, 236, 236, 239, 239, 258,\n",
       "        258, 258, 259, 259, 265, 265, 265, 265, 265, 288, 288, 290, 290,\n",
       "        314, 314, 318, 323, 339, 339, 345, 351, 352, 352, 352, 352, 352,\n",
       "        352, 352, 352, 368, 368, 370, 376, 376, 376, 379, 379, 379, 388,\n",
       "        389, 400, 416, 417, 417, 430, 461, 461, 461, 461, 461, 461, 461,\n",
       "        461, 461, 461, 461, 473, 503, 503, 503, 503, 503, 503, 504, 504,\n",
       "        505, 505, 521, 521, 521, 557, 559, 561, 562, 562, 562, 567, 567,\n",
       "        567, 567, 568], dtype=int64),\n",
       " array([ 6,  5,  6, 10, 19, 25, 26, 29, 30, 26, 27, 30, 12, 13, 16, 18, 20,\n",
       "        26, 30, 24,  9, 29, 30, 29, 16, 19, 26,  9, 16, 17, 18, 27, 10, 15,\n",
       "        16, 20, 26,  6,  7,  9, 19, 29,  1,  3,  4,  6,  7,  8, 23, 12,  5,\n",
       "        30,  6,  7,  8, 13, 16, 27, 17, 20, 15, 19, 29,  3,  4,  5,  6,  7,\n",
       "         8,  9, 11, 12, 13, 14, 15, 16, 17, 19, 11, 19,  9, 19, 29, 20, 30,\n",
       "         7, 10, 16, 17, 18, 20, 16, 20,  1,  3,  4,  8, 21, 23, 24,  6, 26,\n",
       "        16, 19, 26, 29, 30, 12,  7, 25,  1,  3,  4, 11, 13, 14, 19, 15, 16,\n",
       "        17, 18, 20,  2, 22,  2, 21, 24,  2, 22,  6, 11, 13,  2, 22, 14, 21,\n",
       "        22, 23, 24, 16, 18, 16, 20, 15, 19, 10, 29,  4, 24, 15, 19,  1,  3,\n",
       "         4,  7,  8, 21, 23, 24, 14, 24, 29, 10, 17, 20, 25, 26, 30, 20, 18,\n",
       "        27, 12, 11, 13, 27,  1,  3,  4,  7,  8, 11, 13, 14, 21, 23, 24, 12,\n",
       "        11, 13, 14, 21, 23, 24,  5, 10, 10, 15,  3,  4, 24, 12, 12, 12, 26,\n",
       "        27, 30,  6,  7, 26, 27,  5], dtype=int64))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 3\n",
    "np.where(z > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(z < threshold).all(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(495, 31)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitte data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"diagnosis\"], axis = 1)\n",
    "y = data[\"diagnosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier().fit(X_train, y_train)\n",
    "y_pred = dummy_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': '0.532258064516129',\n",
       " 'precision': '0.2978723404255319',\n",
       " 'recall': '0.358974358974359',\n",
       " 'f1': '0.3255813953488372'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = {\n",
    "    'model': [LogisticRegression, SGDClassifier]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model = LogisticRegression\n",
      "\n",
      "model = SGDClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 80.00it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, algorithm in enumerate(tqdm(algorithms[\"model\"])):\n",
    "    y_pred = algorithm().fit(X, y).predict(X_test)\n",
    "    print(f'\\nmodel = {algorithm.__name__}')\n",
    "    scores = evaluate(y_pred, y_test)\n",
    "    results.append(scores[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.9516129032258065', '0.4112903225806452']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto ML (gridsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cv = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = {\n",
    "    'model': [LogisticRegression, SGDClassifier],\n",
    "    'hyperparameters': [{\"C\":np.logspace(-3,3,7),\n",
    "                         \"penalty\":[\"l1\",\"l2\"]}, \n",
    "                       {\n",
    "                        'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3],\n",
    "                        'n_iter': [1000], \n",
    "                        'loss': ['log'], \n",
    "                        'penalty': ['l2'],\n",
    "                        'n_jobs': [-1]}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model = LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████████                                          | 1/2 [00:03<00:03,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model = SGDClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.59s/it]\n"
     ]
    }
   ],
   "source": [
    "for idx, algorithm in enumerate(tqdm(algorithms[\"model\"])):\n",
    "    algorithm_cv = GridSearchCV(algorithm(), algorithms['hyperparameters'][idx])\n",
    "    y_pred = algorithm_cv.fit(X, y).predict(X_test)\n",
    "    print(f'\\nmodel = {algorithm.__name__}')\n",
    "    scores = evaluate(y_pred, y_test)\n",
    "    results_cv.append(scores[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.9838709677419355', '0.9274193548387096']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'økning på 3.2258064516129004 prosent'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'økning på {(float(results_cv[0]) - float(results[0])) * 100} prosent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(32, input_shape = (X_train.shape[1], )))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 32)                992       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 5,217\n",
      "Trainable params: 5,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 371 samples, validate on 124 samples\n",
      "Epoch 1/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0071 - acc: 1.000 - 0s 595us/step - loss: 0.0472 - acc: 0.9784 - val_loss: 0.0581 - val_acc: 0.9677\n",
      "Epoch 2/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0560 - acc: 0.968 - 0s 71us/step - loss: 0.0548 - acc: 0.9784 - val_loss: 0.0625 - val_acc: 0.9758\n",
      "Epoch 3/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1127 - acc: 0.968 - 0s 73us/step - loss: 0.0507 - acc: 0.9811 - val_loss: 0.0652 - val_acc: 0.9758\n",
      "Epoch 4/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0202 - acc: 1.000 - 0s 67us/step - loss: 0.0659 - acc: 0.9704 - val_loss: 0.0754 - val_acc: 0.9677\n",
      "Epoch 5/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0058 - acc: 1.000 - 0s 70us/step - loss: 0.0478 - acc: 0.9757 - val_loss: 0.0653 - val_acc: 0.9758\n",
      "Epoch 6/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0495 - acc: 1.000 - 0s 70us/step - loss: 0.0530 - acc: 0.9784 - val_loss: 0.0592 - val_acc: 0.9839\n",
      "Epoch 7/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0507 - acc: 0.968 - 0s 70us/step - loss: 0.0541 - acc: 0.9838 - val_loss: 0.0730 - val_acc: 0.9597\n",
      "Epoch 8/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0557 - acc: 1.000 - 0s 73us/step - loss: 0.0502 - acc: 0.9838 - val_loss: 0.0385 - val_acc: 0.9919\n",
      "Epoch 9/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0496 - acc: 0.968 - 0s 73us/step - loss: 0.0526 - acc: 0.9730 - val_loss: 0.0849 - val_acc: 0.9516\n",
      "Epoch 10/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0625 - acc: 1.000 - 0s 70us/step - loss: 0.0705 - acc: 0.9677 - val_loss: 0.0673 - val_acc: 0.9677\n",
      "Epoch 11/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0012 - acc: 1.000 - 0s 73us/step - loss: 0.0596 - acc: 0.9730 - val_loss: 0.0527 - val_acc: 0.9839\n",
      "Epoch 12/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0149 - acc: 1.000 - 0s 67us/step - loss: 0.0714 - acc: 0.9704 - val_loss: 0.1090 - val_acc: 0.9597\n",
      "Epoch 13/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1108 - acc: 0.937 - 0s 70us/step - loss: 0.0845 - acc: 0.9677 - val_loss: 0.0634 - val_acc: 0.9758\n",
      "Epoch 14/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0069 - acc: 1.000 - 0s 75us/step - loss: 0.0506 - acc: 0.9757 - val_loss: 0.0807 - val_acc: 0.9516\n",
      "Epoch 15/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0241 - acc: 1.000 - 0s 73us/step - loss: 0.0539 - acc: 0.9784 - val_loss: 0.0689 - val_acc: 0.9758\n",
      "Epoch 16/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0549 - acc: 0.968 - 0s 70us/step - loss: 0.0566 - acc: 0.9757 - val_loss: 0.0694 - val_acc: 0.9677\n",
      "Epoch 17/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0063 - acc: 1.000 - 0s 73us/step - loss: 0.0440 - acc: 0.9838 - val_loss: 0.0565 - val_acc: 0.9839\n",
      "Epoch 18/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0416 - acc: 0.968 - 0s 73us/step - loss: 0.0576 - acc: 0.9704 - val_loss: 0.0776 - val_acc: 0.9516\n",
      "Epoch 19/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0370 - acc: 0.968 - 0s 73us/step - loss: 0.0487 - acc: 0.9730 - val_loss: 0.0649 - val_acc: 0.9758\n",
      "Epoch 20/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0270 - acc: 0.968 - 0s 73us/step - loss: 0.0701 - acc: 0.9704 - val_loss: 0.0990 - val_acc: 0.9597\n",
      "Epoch 21/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0043 - acc: 1.000 - 0s 75us/step - loss: 0.0458 - acc: 0.9784 - val_loss: 0.0494 - val_acc: 0.9758\n",
      "Epoch 22/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0359 - acc: 0.968 - 0s 70us/step - loss: 0.0591 - acc: 0.9704 - val_loss: 0.0525 - val_acc: 0.9839\n",
      "Epoch 23/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0449 - acc: 0.968 - 0s 70us/step - loss: 0.0737 - acc: 0.9704 - val_loss: 0.0845 - val_acc: 0.9597\n",
      "Epoch 24/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0684 - acc: 0.937 - 0s 67us/step - loss: 0.0752 - acc: 0.9704 - val_loss: 0.0559 - val_acc: 0.9758\n",
      "Epoch 25/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.2307 - acc: 0.843 - 0s 70us/step - loss: 0.0611 - acc: 0.9704 - val_loss: 0.0764 - val_acc: 0.9677\n",
      "Epoch 26/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1157 - acc: 0.937 - 0s 65us/step - loss: 0.0755 - acc: 0.9623 - val_loss: 0.0664 - val_acc: 0.9677\n",
      "Epoch 27/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0737 - acc: 0.968 - 0s 73us/step - loss: 0.0823 - acc: 0.9757 - val_loss: 0.0524 - val_acc: 0.9839\n",
      "Epoch 28/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0887 - acc: 0.937 - 0s 70us/step - loss: 0.0630 - acc: 0.9704 - val_loss: 0.0687 - val_acc: 0.9597\n",
      "Epoch 29/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0618 - acc: 0.968 - 0s 84us/step - loss: 0.0870 - acc: 0.9596 - val_loss: 0.0578 - val_acc: 0.9839\n",
      "Epoch 30/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0151 - acc: 1.000 - 0s 73us/step - loss: 0.0864 - acc: 0.9704 - val_loss: 0.0704 - val_acc: 0.9597\n",
      "Epoch 31/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0848 - acc: 0.968 - 0s 78us/step - loss: 0.0686 - acc: 0.9677 - val_loss: 0.0372 - val_acc: 0.9839\n",
      "Epoch 32/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.2033 - acc: 0.906 - 0s 73us/step - loss: 0.0792 - acc: 0.9730 - val_loss: 0.0412 - val_acc: 0.9839\n",
      "Epoch 33/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0060 - acc: 1.000 - 0s 70us/step - loss: 0.0482 - acc: 0.9784 - val_loss: 0.0896 - val_acc: 0.9435\n",
      "Epoch 34/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0398 - acc: 1.000 - 0s 70us/step - loss: 0.0516 - acc: 0.9784 - val_loss: 0.0576 - val_acc: 0.9758\n",
      "Epoch 35/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0704 - acc: 0.968 - 0s 70us/step - loss: 0.0488 - acc: 0.9784 - val_loss: 0.0411 - val_acc: 0.9758\n",
      "Epoch 36/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0167 - acc: 1.000 - 0s 75us/step - loss: 0.0457 - acc: 0.9730 - val_loss: 0.0684 - val_acc: 0.9597\n",
      "Epoch 37/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0628 - acc: 0.968 - 0s 65us/step - loss: 0.0564 - acc: 0.9784 - val_loss: 0.0609 - val_acc: 0.9758\n",
      "Epoch 38/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0737 - acc: 0.968 - 0s 70us/step - loss: 0.0479 - acc: 0.9811 - val_loss: 0.0961 - val_acc: 0.9516\n",
      "Epoch 39/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0640 - acc: 0.968 - 0s 70us/step - loss: 0.0455 - acc: 0.9784 - val_loss: 0.0640 - val_acc: 0.9758\n",
      "Epoch 40/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0489 - acc: 0.968 - 0s 70us/step - loss: 0.0548 - acc: 0.9811 - val_loss: 0.0795 - val_acc: 0.9758\n",
      "Epoch 41/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0026 - acc: 1.000 - 0s 70us/step - loss: 0.0410 - acc: 0.9811 - val_loss: 0.0730 - val_acc: 0.9597\n",
      "Epoch 42/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0580 - acc: 0.968 - 0s 70us/step - loss: 0.0403 - acc: 0.9811 - val_loss: 0.0619 - val_acc: 0.9677\n",
      "Epoch 43/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0633 - acc: 0.968 - 0s 75us/step - loss: 0.0722 - acc: 0.9569 - val_loss: 0.0491 - val_acc: 0.9758\n",
      "Epoch 44/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0347 - acc: 0.968 - 0s 81us/step - loss: 0.0620 - acc: 0.9704 - val_loss: 0.1231 - val_acc: 0.9597\n",
      "Epoch 45/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0499 - acc: 1.000 - 0s 81us/step - loss: 0.0751 - acc: 0.9811 - val_loss: 0.0582 - val_acc: 0.9677\n",
      "Epoch 46/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0525 - acc: 0.968 - 0s 73us/step - loss: 0.0766 - acc: 0.9677 - val_loss: 0.0508 - val_acc: 0.9839\n",
      "Epoch 47/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0067 - acc: 1.000 - 0s 67us/step - loss: 0.0491 - acc: 0.9730 - val_loss: 0.0734 - val_acc: 0.9677\n",
      "Epoch 48/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371/371 [==============================] - ETA: 0s - loss: 0.0305 - acc: 1.000 - 0s 70us/step - loss: 0.0710 - acc: 0.9730 - val_loss: 0.1170 - val_acc: 0.9435\n",
      "Epoch 49/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1097 - acc: 0.937 - 0s 70us/step - loss: 0.0794 - acc: 0.9623 - val_loss: 0.0530 - val_acc: 0.9758\n",
      "Epoch 50/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0064 - acc: 1.000 - 0s 67us/step - loss: 0.0546 - acc: 0.9757 - val_loss: 0.0369 - val_acc: 0.9839\n",
      "Epoch 51/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0138 - acc: 1.000 - 0s 67us/step - loss: 0.0580 - acc: 0.9730 - val_loss: 0.0390 - val_acc: 0.9839\n",
      "Epoch 52/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0012 - acc: 1.000 - 0s 70us/step - loss: 0.0597 - acc: 0.9730 - val_loss: 0.0439 - val_acc: 0.9839\n",
      "Epoch 53/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0226 - acc: 1.000 - 0s 75us/step - loss: 0.0338 - acc: 0.9865 - val_loss: 0.0475 - val_acc: 0.9758\n",
      "Epoch 54/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0770 - acc: 0.968 - 0s 75us/step - loss: 0.0538 - acc: 0.9784 - val_loss: 0.0519 - val_acc: 0.9758\n",
      "Epoch 55/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0364 - acc: 1.000 - 0s 67us/step - loss: 0.0499 - acc: 0.9811 - val_loss: 0.0606 - val_acc: 0.9677\n",
      "Epoch 56/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0603 - acc: 0.968 - 0s 67us/step - loss: 0.0511 - acc: 0.9811 - val_loss: 0.0662 - val_acc: 0.9677\n",
      "Epoch 57/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0458 - acc: 1.000 - 0s 70us/step - loss: 0.0555 - acc: 0.9757 - val_loss: 0.0802 - val_acc: 0.9758\n",
      "Epoch 58/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0393 - acc: 0.968 - 0s 73us/step - loss: 0.0446 - acc: 0.9784 - val_loss: 0.0552 - val_acc: 0.9597\n",
      "Epoch 59/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0302 - acc: 1.000 - 0s 70us/step - loss: 0.0712 - acc: 0.9730 - val_loss: 0.0380 - val_acc: 0.9839\n",
      "Epoch 60/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0609 - acc: 0.968 - 0s 69us/step - loss: 0.0614 - acc: 0.9650 - val_loss: 0.0934 - val_acc: 0.9516\n",
      "Epoch 61/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0145 - acc: 1.000 - 0s 67us/step - loss: 0.0606 - acc: 0.9784 - val_loss: 0.0869 - val_acc: 0.9597\n",
      "Epoch 62/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0530 - acc: 0.968 - 0s 70us/step - loss: 0.0767 - acc: 0.9677 - val_loss: 0.0498 - val_acc: 0.9677\n",
      "Epoch 63/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0135 - acc: 1.000 - 0s 67us/step - loss: 0.0447 - acc: 0.9838 - val_loss: 0.0875 - val_acc: 0.9677\n",
      "Epoch 64/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1126 - acc: 0.968 - 0s 82us/step - loss: 0.0774 - acc: 0.9596 - val_loss: 0.0901 - val_acc: 0.9516\n",
      "Epoch 65/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1109 - acc: 0.937 - 0s 67us/step - loss: 0.0739 - acc: 0.9704 - val_loss: 0.0753 - val_acc: 0.9677\n",
      "Epoch 66/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0673 - acc: 0.968 - 0s 59us/step - loss: 0.0646 - acc: 0.9757 - val_loss: 0.0588 - val_acc: 0.9839\n",
      "Epoch 67/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0112 - acc: 1.000 - 0s 72us/step - loss: 0.0676 - acc: 0.9677 - val_loss: 0.0571 - val_acc: 0.9758\n",
      "Epoch 68/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0368 - acc: 0.968 - 0s 75us/step - loss: 0.0774 - acc: 0.9730 - val_loss: 0.0964 - val_acc: 0.9516\n",
      "Epoch 69/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1540 - acc: 0.937 - 0s 78us/step - loss: 0.0774 - acc: 0.9650 - val_loss: 0.0584 - val_acc: 0.9597\n",
      "Epoch 70/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0218 - acc: 1.000 - 0s 73us/step - loss: 0.0568 - acc: 0.9704 - val_loss: 0.0583 - val_acc: 0.9758\n",
      "Epoch 71/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0706 - acc: 0.937 - 0s 67us/step - loss: 0.0428 - acc: 0.9811 - val_loss: 0.0477 - val_acc: 0.9758\n",
      "Epoch 72/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0440 - acc: 1.000 - 0s 67us/step - loss: 0.0687 - acc: 0.9757 - val_loss: 0.1238 - val_acc: 0.9516\n",
      "Epoch 73/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0994 - acc: 0.968 - 0s 67us/step - loss: 0.0591 - acc: 0.9704 - val_loss: 0.0578 - val_acc: 0.9758\n",
      "Epoch 74/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0204 - acc: 1.000 - 0s 78us/step - loss: 0.0957 - acc: 0.9623 - val_loss: 0.1349 - val_acc: 0.9435\n",
      "Epoch 75/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0976 - acc: 0.937 - 0s 73us/step - loss: 0.1022 - acc: 0.9461 - val_loss: 0.0586 - val_acc: 0.9758\n",
      "Epoch 76/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0899 - acc: 0.968 - 0s 73us/step - loss: 0.0751 - acc: 0.9623 - val_loss: 0.0823 - val_acc: 0.9597\n",
      "Epoch 77/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0177 - acc: 1.000 - 0s 70us/step - loss: 0.0627 - acc: 0.9730 - val_loss: 0.1080 - val_acc: 0.9516\n",
      "Epoch 78/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0185 - acc: 1.000 - 0s 70us/step - loss: 0.0672 - acc: 0.9677 - val_loss: 0.0819 - val_acc: 0.9516\n",
      "Epoch 79/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0120 - acc: 1.000 - 0s 73us/step - loss: 0.0616 - acc: 0.9677 - val_loss: 0.0895 - val_acc: 0.9516\n",
      "Epoch 80/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0482 - acc: 1.000 - 0s 73us/step - loss: 0.0528 - acc: 0.9784 - val_loss: 0.0763 - val_acc: 0.9597\n",
      "Epoch 81/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0820 - acc: 0.968 - 0s 75us/step - loss: 0.0469 - acc: 0.9838 - val_loss: 0.0959 - val_acc: 0.9677\n",
      "Epoch 82/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1106 - acc: 0.937 - 0s 78us/step - loss: 0.0538 - acc: 0.9784 - val_loss: 0.0672 - val_acc: 0.9597\n",
      "Epoch 83/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0132 - acc: 1.000 - 0s 70us/step - loss: 0.0570 - acc: 0.9757 - val_loss: 0.0627 - val_acc: 0.9758\n",
      "Epoch 84/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0235 - acc: 1.000 - 0s 75us/step - loss: 0.0492 - acc: 0.9784 - val_loss: 0.0789 - val_acc: 0.9516\n",
      "Epoch 85/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0084 - acc: 1.000 - 0s 67us/step - loss: 0.0373 - acc: 0.9892 - val_loss: 0.0529 - val_acc: 0.9758\n",
      "Epoch 86/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0741 - acc: 0.968 - 0s 67us/step - loss: 0.0667 - acc: 0.9596 - val_loss: 0.0871 - val_acc: 0.9597\n",
      "Epoch 87/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1603 - acc: 0.937 - 0s 67us/step - loss: 0.0467 - acc: 0.9811 - val_loss: 0.0518 - val_acc: 0.9839\n",
      "Epoch 88/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0908 - acc: 0.968 - 0s 70us/step - loss: 0.0514 - acc: 0.9730 - val_loss: 0.0827 - val_acc: 0.9677\n",
      "Epoch 89/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.2293 - acc: 0.875 - 0s 70us/step - loss: 0.0798 - acc: 0.9542 - val_loss: 0.0651 - val_acc: 0.9758\n",
      "Epoch 90/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1124 - acc: 0.937 - 0s 67us/step - loss: 0.0580 - acc: 0.9730 - val_loss: 0.0627 - val_acc: 0.9677\n",
      "Epoch 91/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0672 - acc: 0.937 - 0s 67us/step - loss: 0.0916 - acc: 0.9623 - val_loss: 0.0757 - val_acc: 0.9597\n",
      "Epoch 92/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.2126 - acc: 0.937 - 0s 73us/step - loss: 0.0642 - acc: 0.9757 - val_loss: 0.0478 - val_acc: 0.9758\n",
      "Epoch 93/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0022 - acc: 1.000 - 0s 70us/step - loss: 0.0526 - acc: 0.9730 - val_loss: 0.0400 - val_acc: 0.9919\n",
      "Epoch 94/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0043 - acc: 1.000 - 0s 70us/step - loss: 0.0716 - acc: 0.9704 - val_loss: 0.1197 - val_acc: 0.9597\n",
      "Epoch 95/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371/371 [==============================] - ETA: 0s - loss: 0.0553 - acc: 0.968 - 0s 65us/step - loss: 0.0650 - acc: 0.9677 - val_loss: 0.0541 - val_acc: 0.9597\n",
      "Epoch 96/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0177 - acc: 1.000 - 0s 69us/step - loss: 0.0618 - acc: 0.9730 - val_loss: 0.0597 - val_acc: 0.9839\n",
      "Epoch 97/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.2440 - acc: 0.937 - 0s 75us/step - loss: 0.0849 - acc: 0.9677 - val_loss: 0.0713 - val_acc: 0.9758\n",
      "Epoch 98/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0312 - acc: 1.000 - 0s 67us/step - loss: 0.0798 - acc: 0.9677 - val_loss: 0.1260 - val_acc: 0.9355\n",
      "Epoch 99/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0665 - acc: 1.000 - 0s 67us/step - loss: 0.0522 - acc: 0.9811 - val_loss: 0.0466 - val_acc: 0.9839\n",
      "Epoch 100/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0361 - acc: 0.968 - 0s 67us/step - loss: 0.0835 - acc: 0.9650 - val_loss: 0.0655 - val_acc: 0.9758\n",
      "Epoch 101/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0483 - acc: 1.000 - 0s 70us/step - loss: 0.0551 - acc: 0.9784 - val_loss: 0.0678 - val_acc: 0.9597\n",
      "Epoch 102/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0239 - acc: 1.000 - 0s 67us/step - loss: 0.0560 - acc: 0.9784 - val_loss: 0.0593 - val_acc: 0.9758\n",
      "Epoch 103/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0044 - acc: 1.000 - 0s 67us/step - loss: 0.0517 - acc: 0.9730 - val_loss: 0.0512 - val_acc: 0.9758\n",
      "Epoch 104/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0892 - acc: 0.968 - 0s 65us/step - loss: 0.0532 - acc: 0.9757 - val_loss: 0.0476 - val_acc: 0.9758\n",
      "Epoch 105/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0656 - acc: 0.968 - 0s 73us/step - loss: 0.0494 - acc: 0.9784 - val_loss: 0.0740 - val_acc: 0.9758\n",
      "Epoch 106/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0138 - acc: 1.000 - 0s 67us/step - loss: 0.0380 - acc: 0.9892 - val_loss: 0.0616 - val_acc: 0.9758\n",
      "Epoch 107/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0711 - acc: 0.937 - 0s 71us/step - loss: 0.0424 - acc: 0.9757 - val_loss: 0.0587 - val_acc: 0.9758\n",
      "Epoch 108/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0382 - acc: 1.000 - 0s 70us/step - loss: 0.0584 - acc: 0.9865 - val_loss: 0.0624 - val_acc: 0.9677\n",
      "Epoch 109/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0176 - acc: 1.000 - 0s 67us/step - loss: 0.0413 - acc: 0.9838 - val_loss: 0.0557 - val_acc: 0.9839\n",
      "Epoch 110/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1275 - acc: 0.906 - 0s 67us/step - loss: 0.0546 - acc: 0.9757 - val_loss: 0.0392 - val_acc: 0.9919\n",
      "Epoch 111/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0146 - acc: 1.000 - 0s 65us/step - loss: 0.0506 - acc: 0.9757 - val_loss: 0.0795 - val_acc: 0.9597\n",
      "Epoch 112/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0722 - acc: 0.937 - 0s 70us/step - loss: 0.0340 - acc: 0.9811 - val_loss: 0.0510 - val_acc: 0.9758\n",
      "Epoch 113/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0435 - acc: 0.968 - 0s 70us/step - loss: 0.0402 - acc: 0.9784 - val_loss: 0.0872 - val_acc: 0.9516\n",
      "Epoch 114/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0692 - acc: 0.968 - 0s 70us/step - loss: 0.0645 - acc: 0.9784 - val_loss: 0.0453 - val_acc: 0.9839\n",
      "Epoch 115/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0297 - acc: 1.000 - 0s 67us/step - loss: 0.0582 - acc: 0.9757 - val_loss: 0.1053 - val_acc: 0.9597\n",
      "Epoch 116/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0299 - acc: 1.000 - 0s 67us/step - loss: 0.0377 - acc: 0.9865 - val_loss: 0.0453 - val_acc: 0.9839\n",
      "Epoch 117/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0018 - acc: 1.000 - 0s 69us/step - loss: 0.0822 - acc: 0.9677 - val_loss: 0.0608 - val_acc: 0.9758\n",
      "Epoch 118/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0395 - acc: 1.000 - 0s 67us/step - loss: 0.0776 - acc: 0.9677 - val_loss: 0.1566 - val_acc: 0.9435\n",
      "Epoch 119/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0163 - acc: 1.000 - 0s 70us/step - loss: 0.1493 - acc: 0.9515 - val_loss: 0.0748 - val_acc: 0.9516\n",
      "Epoch 120/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0283 - acc: 1.000 - 0s 70us/step - loss: 0.1107 - acc: 0.9515 - val_loss: 0.0689 - val_acc: 0.9597\n",
      "Epoch 121/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0584 - acc: 0.968 - 0s 70us/step - loss: 0.0805 - acc: 0.9569 - val_loss: 0.0496 - val_acc: 0.9677\n",
      "Epoch 122/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0038 - acc: 1.000 - 0s 65us/step - loss: 0.0526 - acc: 0.9757 - val_loss: 0.0876 - val_acc: 0.9597\n",
      "Epoch 123/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0267 - acc: 1.000 - 0s 67us/step - loss: 0.0669 - acc: 0.9704 - val_loss: 0.0606 - val_acc: 0.9597\n",
      "Epoch 124/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0505 - acc: 0.968 - 0s 70us/step - loss: 0.0572 - acc: 0.9730 - val_loss: 0.0534 - val_acc: 0.9839\n",
      "Epoch 125/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0424 - acc: 1.000 - 0s 67us/step - loss: 0.0591 - acc: 0.9757 - val_loss: 0.0580 - val_acc: 0.9839\n",
      "Epoch 126/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0132 - acc: 1.000 - 0s 67us/step - loss: 0.0347 - acc: 0.9892 - val_loss: 0.0962 - val_acc: 0.9516\n",
      "Epoch 127/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0450 - acc: 0.968 - 0s 69us/step - loss: 0.0481 - acc: 0.9811 - val_loss: 0.0846 - val_acc: 0.9516\n",
      "Epoch 128/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0996 - acc: 0.968 - 0s 71us/step - loss: 0.0734 - acc: 0.9757 - val_loss: 0.0825 - val_acc: 0.9758\n",
      "Epoch 129/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0625 - acc: 0.968 - 0s 70us/step - loss: 0.0712 - acc: 0.9757 - val_loss: 0.0784 - val_acc: 0.9677\n",
      "Epoch 130/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1030 - acc: 0.937 - 0s 67us/step - loss: 0.0680 - acc: 0.9650 - val_loss: 0.0601 - val_acc: 0.9677\n",
      "Epoch 131/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0428 - acc: 0.968 - 0s 65us/step - loss: 0.0707 - acc: 0.9650 - val_loss: 0.0551 - val_acc: 0.9758\n",
      "Epoch 132/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0403 - acc: 1.000 - 0s 67us/step - loss: 0.0496 - acc: 0.9757 - val_loss: 0.0690 - val_acc: 0.9677\n",
      "Epoch 133/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0963 - acc: 0.968 - 0s 70us/step - loss: 0.0799 - acc: 0.9650 - val_loss: 0.0410 - val_acc: 0.9758\n",
      "Epoch 134/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0123 - acc: 1.000 - 0s 65us/step - loss: 0.0681 - acc: 0.9730 - val_loss: 0.0532 - val_acc: 0.9758\n",
      "Epoch 135/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0337 - acc: 1.000 - 0s 70us/step - loss: 0.0521 - acc: 0.9811 - val_loss: 0.0637 - val_acc: 0.9677\n",
      "Epoch 136/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0215 - acc: 1.000 - 0s 67us/step - loss: 0.0831 - acc: 0.9677 - val_loss: 0.0485 - val_acc: 0.9839\n",
      "Epoch 137/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0012 - acc: 1.000 - 0s 67us/step - loss: 0.0736 - acc: 0.9596 - val_loss: 0.1209 - val_acc: 0.9516\n",
      "Epoch 138/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0280 - acc: 1.000 - 0s 67us/step - loss: 0.0765 - acc: 0.9677 - val_loss: 0.0511 - val_acc: 0.9758\n",
      "Epoch 139/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0230 - acc: 1.000 - 0s 67us/step - loss: 0.0544 - acc: 0.9784 - val_loss: 0.0598 - val_acc: 0.9677\n",
      "Epoch 140/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0488 - acc: 0.968 - 0s 65us/step - loss: 0.0356 - acc: 0.9865 - val_loss: 0.0743 - val_acc: 0.9677\n",
      "Epoch 141/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0361 - acc: 0.968 - 0s 70us/step - loss: 0.0592 - acc: 0.9784 - val_loss: 0.0620 - val_acc: 0.9597\n",
      "Epoch 142/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371/371 [==============================] - ETA: 0s - loss: 0.0329 - acc: 1.000 - 0s 67us/step - loss: 0.0536 - acc: 0.9811 - val_loss: 0.0577 - val_acc: 0.9758\n",
      "Epoch 143/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0685 - acc: 0.968 - 0s 70us/step - loss: 0.0552 - acc: 0.9730 - val_loss: 0.0418 - val_acc: 0.9839\n",
      "Epoch 144/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0526 - acc: 0.968 - 0s 70us/step - loss: 0.0555 - acc: 0.9704 - val_loss: 0.0846 - val_acc: 0.9677\n",
      "Epoch 145/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0247 - acc: 1.000 - 0s 66us/step - loss: 0.0566 - acc: 0.9784 - val_loss: 0.0595 - val_acc: 0.9758\n",
      "Epoch 146/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0205 - acc: 1.000 - 0s 67us/step - loss: 0.0497 - acc: 0.9811 - val_loss: 0.0418 - val_acc: 0.9839\n",
      "Epoch 147/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0676 - acc: 0.968 - 0s 70us/step - loss: 0.0538 - acc: 0.9757 - val_loss: 0.0533 - val_acc: 0.9758\n",
      "Epoch 148/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0270 - acc: 1.000 - 0s 67us/step - loss: 0.0348 - acc: 0.9838 - val_loss: 0.0534 - val_acc: 0.9839\n",
      "Epoch 149/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0824 - acc: 0.937 - 0s 67us/step - loss: 0.0508 - acc: 0.9730 - val_loss: 0.0605 - val_acc: 0.9677\n",
      "Epoch 150/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0091 - acc: 1.000 - 0s 70us/step - loss: 0.0473 - acc: 0.9784 - val_loss: 0.1043 - val_acc: 0.9597\n",
      "Epoch 151/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0525 - acc: 0.968 - 0s 67us/step - loss: 0.0772 - acc: 0.9650 - val_loss: 0.0916 - val_acc: 0.9597\n",
      "Epoch 152/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.3742 - acc: 0.875 - 0s 65us/step - loss: 0.0991 - acc: 0.9650 - val_loss: 0.0720 - val_acc: 0.9677\n",
      "Epoch 153/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1031 - acc: 0.937 - 0s 70us/step - loss: 0.1001 - acc: 0.9515 - val_loss: 0.0400 - val_acc: 0.9839\n",
      "Epoch 154/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1519 - acc: 0.937 - 0s 67us/step - loss: 0.0840 - acc: 0.9596 - val_loss: 0.0621 - val_acc: 0.9758\n",
      "Epoch 155/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0367 - acc: 0.968 - 0s 70us/step - loss: 0.0735 - acc: 0.9650 - val_loss: 0.0693 - val_acc: 0.9677\n",
      "Epoch 156/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0444 - acc: 0.968 - 0s 65us/step - loss: 0.0553 - acc: 0.9757 - val_loss: 0.0603 - val_acc: 0.9597\n",
      "Epoch 157/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0658 - acc: 0.937 - 0s 67us/step - loss: 0.0594 - acc: 0.9757 - val_loss: 0.0690 - val_acc: 0.9758\n",
      "Epoch 158/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0142 - acc: 1.000 - 0s 70us/step - loss: 0.0620 - acc: 0.9704 - val_loss: 0.0814 - val_acc: 0.9597\n",
      "Epoch 159/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0761 - acc: 0.937 - 0s 70us/step - loss: 0.0632 - acc: 0.9704 - val_loss: 0.0507 - val_acc: 0.9758\n",
      "Epoch 160/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0268 - acc: 1.000 - 0s 70us/step - loss: 0.0512 - acc: 0.9757 - val_loss: 0.0860 - val_acc: 0.9516\n",
      "Epoch 161/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0351 - acc: 1.000 - 0s 67us/step - loss: 0.0556 - acc: 0.9811 - val_loss: 0.0455 - val_acc: 0.9677\n",
      "Epoch 162/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0540 - acc: 0.968 - 0s 70us/step - loss: 0.0837 - acc: 0.9650 - val_loss: 0.0811 - val_acc: 0.9677\n",
      "Epoch 163/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0423 - acc: 0.968 - 0s 67us/step - loss: 0.1224 - acc: 0.9434 - val_loss: 0.0889 - val_acc: 0.9597\n",
      "Epoch 164/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1110 - acc: 0.937 - 0s 65us/step - loss: 0.0587 - acc: 0.9730 - val_loss: 0.0746 - val_acc: 0.9677\n",
      "Epoch 165/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0523 - acc: 0.968 - 0s 70us/step - loss: 0.0614 - acc: 0.9757 - val_loss: 0.0586 - val_acc: 0.9758\n",
      "Epoch 166/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0495 - acc: 0.968 - 0s 67us/step - loss: 0.0554 - acc: 0.9623 - val_loss: 0.0687 - val_acc: 0.9597\n",
      "Epoch 167/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0394 - acc: 0.968 - 0s 70us/step - loss: 0.0498 - acc: 0.9811 - val_loss: 0.0654 - val_acc: 0.9597\n",
      "Epoch 168/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0044 - acc: 1.000 - 0s 67us/step - loss: 0.0435 - acc: 0.9811 - val_loss: 0.0655 - val_acc: 0.9758\n",
      "Epoch 169/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0758 - acc: 0.968 - 0s 69us/step - loss: 0.0335 - acc: 0.9838 - val_loss: 0.0592 - val_acc: 0.9758\n",
      "Epoch 170/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0325 - acc: 1.000 - 0s 67us/step - loss: 0.0586 - acc: 0.9811 - val_loss: 0.1087 - val_acc: 0.9516\n",
      "Epoch 171/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0100 - acc: 1.000 - 0s 67us/step - loss: 0.0656 - acc: 0.9623 - val_loss: 0.0838 - val_acc: 0.9677\n",
      "Epoch 172/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0027 - acc: 1.000 - 0s 62us/step - loss: 0.0690 - acc: 0.9704 - val_loss: 0.0656 - val_acc: 0.9758\n",
      "Epoch 173/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1820 - acc: 0.968 - 0s 67us/step - loss: 0.0969 - acc: 0.9677 - val_loss: 0.1278 - val_acc: 0.9435\n",
      "Epoch 174/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0260 - acc: 1.000 - 0s 70us/step - loss: 0.0626 - acc: 0.9677 - val_loss: 0.0516 - val_acc: 0.9758\n",
      "Epoch 175/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0594 - acc: 0.968 - 0s 70us/step - loss: 0.0755 - acc: 0.9677 - val_loss: 0.0683 - val_acc: 0.9758\n",
      "Epoch 176/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0099 - acc: 1.000 - 0s 62us/step - loss: 0.0674 - acc: 0.9730 - val_loss: 0.0681 - val_acc: 0.9758\n",
      "Epoch 177/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0861 - acc: 0.968 - 0s 70us/step - loss: 0.0627 - acc: 0.9677 - val_loss: 0.0555 - val_acc: 0.9758\n",
      "Epoch 178/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0431 - acc: 0.968 - 0s 71us/step - loss: 0.0496 - acc: 0.9757 - val_loss: 0.0596 - val_acc: 0.9758\n",
      "Epoch 179/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0268 - acc: 1.000 - 0s 67us/step - loss: 0.0422 - acc: 0.9865 - val_loss: 0.0759 - val_acc: 0.9597\n",
      "Epoch 180/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0721 - acc: 0.968 - 0s 67us/step - loss: 0.0463 - acc: 0.9838 - val_loss: 0.0480 - val_acc: 0.9758\n",
      "Epoch 181/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0274 - acc: 1.000 - 0s 67us/step - loss: 0.0589 - acc: 0.9811 - val_loss: 0.0830 - val_acc: 0.9758\n",
      "Epoch 182/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0121 - acc: 1.000 - 0s 73us/step - loss: 0.0541 - acc: 0.9784 - val_loss: 0.0759 - val_acc: 0.9677\n",
      "Epoch 183/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0484 - acc: 0.968 - 0s 70us/step - loss: 0.0784 - acc: 0.9677 - val_loss: 0.0904 - val_acc: 0.9597\n",
      "Epoch 184/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0628 - acc: 0.968 - 0s 67us/step - loss: 0.0629 - acc: 0.9784 - val_loss: 0.0623 - val_acc: 0.9677\n",
      "Epoch 185/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0963 - acc: 0.968 - 0s 70us/step - loss: 0.0653 - acc: 0.9650 - val_loss: 0.0607 - val_acc: 0.9677\n",
      "Epoch 186/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0641 - acc: 1.000 - 0s 70us/step - loss: 0.0671 - acc: 0.9704 - val_loss: 0.0505 - val_acc: 0.9758\n",
      "Epoch 187/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0956 - acc: 0.937 - 0s 70us/step - loss: 0.0606 - acc: 0.9757 - val_loss: 0.0643 - val_acc: 0.9597\n",
      "Epoch 188/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0530 - acc: 1.000 - 0s 67us/step - loss: 0.0499 - acc: 0.9838 - val_loss: 0.0851 - val_acc: 0.9516\n",
      "Epoch 189/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371/371 [==============================] - ETA: 0s - loss: 0.0195 - acc: 1.000 - 0s 70us/step - loss: 0.0500 - acc: 0.9677 - val_loss: 0.0563 - val_acc: 0.9839\n",
      "Epoch 190/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0354 - acc: 1.000 - 0s 75us/step - loss: 0.0599 - acc: 0.9704 - val_loss: 0.0858 - val_acc: 0.9516\n",
      "Epoch 191/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0403 - acc: 1.000 - 0s 81us/step - loss: 0.0650 - acc: 0.9623 - val_loss: 0.0778 - val_acc: 0.9597\n",
      "Epoch 192/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0080 - acc: 1.000 - 0s 70us/step - loss: 0.0745 - acc: 0.9784 - val_loss: 0.0826 - val_acc: 0.9597\n",
      "Epoch 193/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0256 - acc: 1.000 - 0s 73us/step - loss: 0.0583 - acc: 0.9784 - val_loss: 0.0627 - val_acc: 0.9758\n",
      "Epoch 194/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0376 - acc: 0.968 - 0s 70us/step - loss: 0.0475 - acc: 0.9811 - val_loss: 0.0781 - val_acc: 0.9597\n",
      "Epoch 195/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0052 - acc: 1.000 - 0s 67us/step - loss: 0.0423 - acc: 0.9811 - val_loss: 0.0479 - val_acc: 0.9839\n",
      "Epoch 196/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0716 - acc: 0.968 - 0s 67us/step - loss: 0.0416 - acc: 0.9811 - val_loss: 0.0922 - val_acc: 0.9516\n",
      "Epoch 197/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0493 - acc: 1.000 - 0s 73us/step - loss: 0.0549 - acc: 0.9784 - val_loss: 0.0564 - val_acc: 0.9758\n",
      "Epoch 198/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.2232 - acc: 0.906 - 0s 67us/step - loss: 0.0718 - acc: 0.9677 - val_loss: 0.0411 - val_acc: 0.9758\n",
      "Epoch 199/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0525 - acc: 0.937 - 0s 65us/step - loss: 0.0711 - acc: 0.9569 - val_loss: 0.0524 - val_acc: 0.9758\n",
      "Epoch 200/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0198 - acc: 1.000 - 0s 70us/step - loss: 0.0518 - acc: 0.9757 - val_loss: 0.0874 - val_acc: 0.9516\n",
      "Epoch 201/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1197 - acc: 0.968 - 0s 67us/step - loss: 0.0749 - acc: 0.9677 - val_loss: 0.0690 - val_acc: 0.9516\n",
      "Epoch 202/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0319 - acc: 1.000 - 0s 67us/step - loss: 0.0720 - acc: 0.9784 - val_loss: 0.0506 - val_acc: 0.9597\n",
      "Epoch 203/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 8.1102e-04 - acc: 1.000 - 0s 67us/step - loss: 0.0516 - acc: 0.9811 - val_loss: 0.0778 - val_acc: 0.9597\n",
      "Epoch 204/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0808 - acc: 0.968 - 0s 67us/step - loss: 0.0485 - acc: 0.9757 - val_loss: 0.0498 - val_acc: 0.9839\n",
      "Epoch 205/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1219 - acc: 0.937 - 0s 70us/step - loss: 0.0528 - acc: 0.9757 - val_loss: 0.0564 - val_acc: 0.9758\n",
      "Epoch 206/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0608 - acc: 0.968 - 0s 67us/step - loss: 0.0412 - acc: 0.9865 - val_loss: 0.0513 - val_acc: 0.9597\n",
      "Epoch 207/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0255 - acc: 1.000 - 0s 67us/step - loss: 0.0477 - acc: 0.9811 - val_loss: 0.0335 - val_acc: 0.9919\n",
      "Epoch 208/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0554 - acc: 0.968 - 0s 70us/step - loss: 0.0459 - acc: 0.9784 - val_loss: 0.0507 - val_acc: 0.9677\n",
      "Epoch 209/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0196 - acc: 1.000 - 0s 71us/step - loss: 0.0582 - acc: 0.9730 - val_loss: 0.0870 - val_acc: 0.9516\n",
      "Epoch 210/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0985 - acc: 0.968 - 0s 70us/step - loss: 0.0497 - acc: 0.9757 - val_loss: 0.0634 - val_acc: 0.9677\n",
      "Epoch 211/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0060 - acc: 1.000 - 0s 74us/step - loss: 0.0463 - acc: 0.9811 - val_loss: 0.0746 - val_acc: 0.9758\n",
      "Epoch 212/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0064 - acc: 1.000 - 0s 70us/step - loss: 0.0496 - acc: 0.9784 - val_loss: 0.0560 - val_acc: 0.9758\n",
      "Epoch 213/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0082 - acc: 1.000 - 0s 70us/step - loss: 0.0291 - acc: 0.9865 - val_loss: 0.0520 - val_acc: 0.9839\n",
      "Epoch 214/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0258 - acc: 1.000 - 0s 70us/step - loss: 0.0357 - acc: 0.9892 - val_loss: 0.0931 - val_acc: 0.9516\n",
      "Epoch 215/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0933 - acc: 0.968 - 0s 70us/step - loss: 0.0570 - acc: 0.9811 - val_loss: 0.0568 - val_acc: 0.9839\n",
      "Epoch 216/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1505 - acc: 0.968 - 0s 73us/step - loss: 0.0658 - acc: 0.9730 - val_loss: 0.0459 - val_acc: 0.9677\n",
      "Epoch 217/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0100 - acc: 1.000 - 0s 70us/step - loss: 0.0620 - acc: 0.9730 - val_loss: 0.0878 - val_acc: 0.9516\n",
      "Epoch 218/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0336 - acc: 1.000 - 0s 70us/step - loss: 0.0508 - acc: 0.9784 - val_loss: 0.0471 - val_acc: 0.9839\n",
      "Epoch 219/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0183 - acc: 1.000 - 0s 70us/step - loss: 0.0601 - acc: 0.9704 - val_loss: 0.0490 - val_acc: 0.9758\n",
      "Epoch 220/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0497 - acc: 0.968 - 0s 73us/step - loss: 0.0514 - acc: 0.9838 - val_loss: 0.0486 - val_acc: 0.9758\n",
      "Epoch 221/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0050 - acc: 1.000 - 0s 74us/step - loss: 0.0526 - acc: 0.9757 - val_loss: 0.0562 - val_acc: 0.9677\n",
      "Epoch 222/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0653 - acc: 0.968 - 0s 73us/step - loss: 0.0515 - acc: 0.9730 - val_loss: 0.0624 - val_acc: 0.9597\n",
      "Epoch 223/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0413 - acc: 1.000 - 0s 70us/step - loss: 0.0405 - acc: 0.9865 - val_loss: 0.1106 - val_acc: 0.9516\n",
      "Epoch 224/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0403 - acc: 0.968 - 0s 70us/step - loss: 0.0390 - acc: 0.9811 - val_loss: 0.0564 - val_acc: 0.9758\n",
      "Epoch 225/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0219 - acc: 1.000 - 0s 70us/step - loss: 0.0357 - acc: 0.9865 - val_loss: 0.0656 - val_acc: 0.9758\n",
      "Epoch 226/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0053 - acc: 1.000 - 0s 75us/step - loss: 0.0354 - acc: 0.9865 - val_loss: 0.0830 - val_acc: 0.9677\n",
      "Epoch 227/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0220 - acc: 1.000 - 0s 70us/step - loss: 0.0381 - acc: 0.9865 - val_loss: 0.1065 - val_acc: 0.9516\n",
      "Epoch 228/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1055 - acc: 0.968 - 0s 67us/step - loss: 0.0595 - acc: 0.9757 - val_loss: 0.0677 - val_acc: 0.9677\n",
      "Epoch 229/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0378 - acc: 1.000 - 0s 70us/step - loss: 0.0700 - acc: 0.9677 - val_loss: 0.0414 - val_acc: 0.9839\n",
      "Epoch 230/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0185 - acc: 1.000 - 0s 67us/step - loss: 0.0539 - acc: 0.9757 - val_loss: 0.1068 - val_acc: 0.9597\n",
      "Epoch 231/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0927 - acc: 0.937 - 0s 73us/step - loss: 0.0664 - acc: 0.9730 - val_loss: 0.0893 - val_acc: 0.9758\n",
      "Epoch 232/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0043 - acc: 1.000 - 0s 67us/step - loss: 0.0584 - acc: 0.9784 - val_loss: 0.0563 - val_acc: 0.9677\n",
      "Epoch 233/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1186 - acc: 0.937 - 0s 70us/step - loss: 0.0658 - acc: 0.9704 - val_loss: 0.1137 - val_acc: 0.9516\n",
      "Epoch 234/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0048 - acc: 1.000 - 0s 67us/step - loss: 0.0905 - acc: 0.9650 - val_loss: 0.0493 - val_acc: 0.9839\n",
      "Epoch 235/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0401 - acc: 0.968 - 0s 70us/step - loss: 0.0427 - acc: 0.9838 - val_loss: 0.0785 - val_acc: 0.9597\n",
      "Epoch 236/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371/371 [==============================] - ETA: 0s - loss: 0.1173 - acc: 0.937 - 0s 65us/step - loss: 0.0451 - acc: 0.9757 - val_loss: 0.0433 - val_acc: 0.9839\n",
      "Epoch 237/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0026 - acc: 1.000 - 0s 67us/step - loss: 0.0390 - acc: 0.9784 - val_loss: 0.0530 - val_acc: 0.9597\n",
      "Epoch 238/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0649 - acc: 0.968 - 0s 73us/step - loss: 0.0483 - acc: 0.9811 - val_loss: 0.0435 - val_acc: 0.9839\n",
      "Epoch 239/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0587 - acc: 0.968 - 0s 70us/step - loss: 0.0613 - acc: 0.9730 - val_loss: 0.0614 - val_acc: 0.9758\n",
      "Epoch 240/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0441 - acc: 1.000 - 0s 65us/step - loss: 0.0690 - acc: 0.9730 - val_loss: 0.0861 - val_acc: 0.9597\n",
      "Epoch 241/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1036 - acc: 0.968 - 0s 67us/step - loss: 0.0653 - acc: 0.9784 - val_loss: 0.0967 - val_acc: 0.9677\n",
      "Epoch 242/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0310 - acc: 1.000 - 0s 67us/step - loss: 0.0534 - acc: 0.9811 - val_loss: 0.0452 - val_acc: 0.9758\n",
      "Epoch 243/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0203 - acc: 1.000 - 0s 70us/step - loss: 0.0679 - acc: 0.9757 - val_loss: 0.1645 - val_acc: 0.9355\n",
      "Epoch 244/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0615 - acc: 0.968 - 0s 67us/step - loss: 0.0769 - acc: 0.9677 - val_loss: 0.1050 - val_acc: 0.9516\n",
      "Epoch 245/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1387 - acc: 0.906 - 0s 67us/step - loss: 0.0760 - acc: 0.9596 - val_loss: 0.0537 - val_acc: 0.9677\n",
      "Epoch 246/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0408 - acc: 0.968 - 0s 67us/step - loss: 0.0851 - acc: 0.9596 - val_loss: 0.0465 - val_acc: 0.9919\n",
      "Epoch 247/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.2644 - acc: 0.937 - 0s 67us/step - loss: 0.0832 - acc: 0.9757 - val_loss: 0.0622 - val_acc: 0.9677\n",
      "Epoch 248/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0430 - acc: 0.968 - 0s 67us/step - loss: 0.0455 - acc: 0.9784 - val_loss: 0.0405 - val_acc: 0.9839\n",
      "Epoch 249/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1076 - acc: 0.968 - 0s 70us/step - loss: 0.0571 - acc: 0.9704 - val_loss: 0.0320 - val_acc: 0.9919\n",
      "Epoch 250/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0916 - acc: 0.937 - 0s 73us/step - loss: 0.0484 - acc: 0.9677 - val_loss: 0.0782 - val_acc: 0.9516\n",
      "Epoch 251/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0310 - acc: 1.000 - 0s 67us/step - loss: 0.0702 - acc: 0.9730 - val_loss: 0.0554 - val_acc: 0.9677\n",
      "Epoch 252/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1955 - acc: 0.906 - 0s 70us/step - loss: 0.0862 - acc: 0.9623 - val_loss: 0.0338 - val_acc: 0.9839\n",
      "Epoch 253/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0186 - acc: 1.000 - 0s 69us/step - loss: 0.0594 - acc: 0.9677 - val_loss: 0.0926 - val_acc: 0.9435\n",
      "Epoch 254/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0825 - acc: 0.968 - 0s 70us/step - loss: 0.0645 - acc: 0.9677 - val_loss: 0.0686 - val_acc: 0.9758\n",
      "Epoch 255/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0521 - acc: 0.937 - 0s 67us/step - loss: 0.0356 - acc: 0.9784 - val_loss: 0.0415 - val_acc: 0.9839\n",
      "Epoch 256/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0553 - acc: 0.968 - 0s 70us/step - loss: 0.0611 - acc: 0.9730 - val_loss: 0.0342 - val_acc: 0.9839\n",
      "Epoch 257/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0248 - acc: 1.000 - 0s 70us/step - loss: 0.0506 - acc: 0.9811 - val_loss: 0.0675 - val_acc: 0.9758\n",
      "Epoch 258/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0026 - acc: 1.000 - 0s 67us/step - loss: 0.0477 - acc: 0.9784 - val_loss: 0.0409 - val_acc: 0.9677\n",
      "Epoch 259/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0176 - acc: 1.000 - 0s 70us/step - loss: 0.0551 - acc: 0.9757 - val_loss: 0.0958 - val_acc: 0.9597\n",
      "Epoch 260/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0368 - acc: 1.000 - 0s 70us/step - loss: 0.0473 - acc: 0.9784 - val_loss: 0.0408 - val_acc: 0.9919\n",
      "Epoch 261/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0694 - acc: 0.968 - 0s 69us/step - loss: 0.0680 - acc: 0.9704 - val_loss: 0.0620 - val_acc: 0.9516\n",
      "Epoch 262/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0363 - acc: 0.968 - 0s 67us/step - loss: 0.0431 - acc: 0.9865 - val_loss: 0.0501 - val_acc: 0.9677\n",
      "Epoch 263/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0046 - acc: 1.000 - 0s 70us/step - loss: 0.0462 - acc: 0.9757 - val_loss: 0.0879 - val_acc: 0.9516\n",
      "Epoch 264/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0094 - acc: 1.000 - 0s 67us/step - loss: 0.0463 - acc: 0.9811 - val_loss: 0.0546 - val_acc: 0.9839\n",
      "Epoch 265/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0070 - acc: 1.000 - 0s 67us/step - loss: 0.0656 - acc: 0.9730 - val_loss: 0.0672 - val_acc: 0.9677\n",
      "Epoch 266/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0345 - acc: 1.000 - 0s 71us/step - loss: 0.0580 - acc: 0.9811 - val_loss: 0.0441 - val_acc: 0.9919\n",
      "Epoch 267/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0540 - acc: 0.968 - 0s 70us/step - loss: 0.0412 - acc: 0.9838 - val_loss: 0.0383 - val_acc: 0.9919\n",
      "Epoch 268/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0919 - acc: 0.937 - 0s 70us/step - loss: 0.0466 - acc: 0.9757 - val_loss: 0.0803 - val_acc: 0.9597\n",
      "Epoch 269/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0764 - acc: 0.937 - 0s 70us/step - loss: 0.0600 - acc: 0.9730 - val_loss: 0.0617 - val_acc: 0.9758\n",
      "Epoch 270/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0335 - acc: 1.000 - 0s 75us/step - loss: 0.0562 - acc: 0.9811 - val_loss: 0.0671 - val_acc: 0.9758\n",
      "Epoch 271/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0247 - acc: 1.000 - 0s 67us/step - loss: 0.0396 - acc: 0.9865 - val_loss: 0.0549 - val_acc: 0.9677\n",
      "Epoch 272/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1315 - acc: 0.937 - 0s 67us/step - loss: 0.0552 - acc: 0.9730 - val_loss: 0.1148 - val_acc: 0.9597\n",
      "Epoch 273/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0526 - acc: 0.968 - 0s 70us/step - loss: 0.0475 - acc: 0.9811 - val_loss: 0.0475 - val_acc: 0.9839\n",
      "Epoch 274/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0794 - acc: 0.968 - 0s 67us/step - loss: 0.0443 - acc: 0.9784 - val_loss: 0.0626 - val_acc: 0.9758\n",
      "Epoch 275/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1374 - acc: 0.968 - 0s 70us/step - loss: 0.0563 - acc: 0.9704 - val_loss: 0.0347 - val_acc: 0.9919\n",
      "Epoch 276/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0524 - acc: 0.968 - 0s 78us/step - loss: 0.0309 - acc: 0.9811 - val_loss: 0.0791 - val_acc: 0.9516\n",
      "Epoch 277/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1057 - acc: 0.937 - 0s 78us/step - loss: 0.0641 - acc: 0.9730 - val_loss: 0.0495 - val_acc: 0.9677\n",
      "Epoch 278/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0792 - acc: 0.968 - 0s 70us/step - loss: 0.0604 - acc: 0.9811 - val_loss: 0.0348 - val_acc: 0.9839\n",
      "Epoch 279/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0134 - acc: 1.000 - 0s 69us/step - loss: 0.0407 - acc: 0.9865 - val_loss: 0.0427 - val_acc: 0.9839\n",
      "Epoch 280/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1794 - acc: 0.937 - 0s 67us/step - loss: 0.0624 - acc: 0.9757 - val_loss: 0.0531 - val_acc: 0.9839\n",
      "Epoch 281/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0070 - acc: 1.000 - 0s 65us/step - loss: 0.0541 - acc: 0.9730 - val_loss: 0.0403 - val_acc: 0.9839\n",
      "Epoch 282/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0156 - acc: 1.000 - 0s 67us/step - loss: 0.0444 - acc: 0.9811 - val_loss: 0.0310 - val_acc: 0.9839\n",
      "Epoch 283/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371/371 [==============================] - ETA: 0s - loss: 0.0351 - acc: 1.000 - 0s 73us/step - loss: 0.0434 - acc: 0.9811 - val_loss: 0.0319 - val_acc: 0.9919\n",
      "Epoch 284/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0560 - acc: 0.968 - 0s 73us/step - loss: 0.0589 - acc: 0.9757 - val_loss: 0.0522 - val_acc: 0.9839\n",
      "Epoch 285/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0563 - acc: 0.968 - 0s 78us/step - loss: 0.0394 - acc: 0.9784 - val_loss: 0.0651 - val_acc: 0.9677\n",
      "Epoch 286/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 5.5963e-04 - acc: 1.000 - 0s 73us/step - loss: 0.0480 - acc: 0.9784 - val_loss: 0.0343 - val_acc: 0.9919\n",
      "Epoch 287/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0502 - acc: 0.968 - 0s 70us/step - loss: 0.0545 - acc: 0.9757 - val_loss: 0.1149 - val_acc: 0.9516\n",
      "Epoch 288/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.2175 - acc: 0.875 - 0s 67us/step - loss: 0.0883 - acc: 0.9650 - val_loss: 0.0539 - val_acc: 0.9758\n",
      "Epoch 289/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0386 - acc: 1.000 - 0s 65us/step - loss: 0.0504 - acc: 0.9784 - val_loss: 0.0486 - val_acc: 0.9839\n",
      "Epoch 290/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0442 - acc: 0.968 - 0s 65us/step - loss: 0.0645 - acc: 0.9730 - val_loss: 0.0848 - val_acc: 0.9758\n",
      "Epoch 291/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1443 - acc: 0.906 - 0s 67us/step - loss: 0.0760 - acc: 0.9757 - val_loss: 0.0875 - val_acc: 0.9677\n",
      "Epoch 292/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0534 - acc: 0.968 - 0s 65us/step - loss: 0.0589 - acc: 0.9730 - val_loss: 0.0695 - val_acc: 0.9758\n",
      "Epoch 293/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0120 - acc: 1.000 - 0s 67us/step - loss: 0.0352 - acc: 0.9838 - val_loss: 0.0455 - val_acc: 0.9758\n",
      "Epoch 294/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0474 - acc: 0.968 - 0s 70us/step - loss: 0.0641 - acc: 0.9623 - val_loss: 0.0498 - val_acc: 0.9839\n",
      "Epoch 295/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0586 - acc: 0.968 - 0s 70us/step - loss: 0.0498 - acc: 0.9757 - val_loss: 0.0655 - val_acc: 0.9839\n",
      "Epoch 296/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0085 - acc: 1.000 - 0s 67us/step - loss: 0.0331 - acc: 0.9865 - val_loss: 0.0706 - val_acc: 0.9677\n",
      "Epoch 297/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0243 - acc: 0.968 - 0s 67us/step - loss: 0.0404 - acc: 0.9730 - val_loss: 0.0365 - val_acc: 0.9919\n",
      "Epoch 298/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0884 - acc: 0.968 - 0s 66us/step - loss: 0.0616 - acc: 0.9811 - val_loss: 0.1422 - val_acc: 0.9355\n",
      "Epoch 299/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0031 - acc: 1.000 - 0s 65us/step - loss: 0.0837 - acc: 0.9757 - val_loss: 0.0599 - val_acc: 0.9758\n",
      "Epoch 300/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0266 - acc: 1.000 - 0s 67us/step - loss: 0.0403 - acc: 0.9865 - val_loss: 0.0765 - val_acc: 0.9597\n",
      "Epoch 301/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0625 - acc: 0.968 - 0s 65us/step - loss: 0.0534 - acc: 0.9784 - val_loss: 0.0662 - val_acc: 0.9677\n",
      "Epoch 302/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0544 - acc: 1.000 - 0s 65us/step - loss: 0.0460 - acc: 0.9784 - val_loss: 0.0642 - val_acc: 0.9758\n",
      "Epoch 303/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0723 - acc: 0.968 - 0s 70us/step - loss: 0.0427 - acc: 0.9811 - val_loss: 0.0432 - val_acc: 0.9677\n",
      "Epoch 304/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0047 - acc: 1.000 - 0s 89us/step - loss: 0.0336 - acc: 0.9865 - val_loss: 0.0548 - val_acc: 0.9597\n",
      "Epoch 305/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0423 - acc: 0.968 - 0s 75us/step - loss: 0.0532 - acc: 0.9811 - val_loss: 0.0505 - val_acc: 0.9839\n",
      "Epoch 306/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 1.0916e-04 - acc: 1.000 - 0s 70us/step - loss: 0.0463 - acc: 0.9784 - val_loss: 0.0375 - val_acc: 0.9758\n",
      "Epoch 307/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0294 - acc: 1.000 - 0s 69us/step - loss: 0.0411 - acc: 0.9811 - val_loss: 0.0589 - val_acc: 0.9597\n",
      "Epoch 308/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0141 - acc: 1.000 - 0s 70us/step - loss: 0.0530 - acc: 0.9757 - val_loss: 0.0794 - val_acc: 0.9597\n",
      "Epoch 309/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0299 - acc: 1.000 - 0s 67us/step - loss: 0.0592 - acc: 0.9784 - val_loss: 0.0917 - val_acc: 0.9597\n",
      "Epoch 310/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0767 - acc: 0.968 - 0s 70us/step - loss: 0.0574 - acc: 0.9704 - val_loss: 0.0650 - val_acc: 0.9758\n",
      "Epoch 311/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0096 - acc: 1.000 - 0s 70us/step - loss: 0.1332 - acc: 0.9542 - val_loss: 0.1112 - val_acc: 0.9516\n",
      "Epoch 312/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0806 - acc: 0.937 - 0s 70us/step - loss: 0.0898 - acc: 0.9596 - val_loss: 0.0489 - val_acc: 0.9758\n",
      "Epoch 313/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1760 - acc: 0.875 - 0s 73us/step - loss: 0.0648 - acc: 0.9677 - val_loss: 0.0546 - val_acc: 0.9758\n",
      "Epoch 314/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0326 - acc: 1.000 - 0s 70us/step - loss: 0.0635 - acc: 0.9650 - val_loss: 0.0452 - val_acc: 0.9839\n",
      "Epoch 315/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0057 - acc: 1.000 - 0s 73us/step - loss: 0.0702 - acc: 0.9730 - val_loss: 0.0871 - val_acc: 0.9597\n",
      "Epoch 316/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0344 - acc: 0.968 - 0s 67us/step - loss: 0.0398 - acc: 0.9757 - val_loss: 0.0404 - val_acc: 0.9839\n",
      "Epoch 317/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0296 - acc: 1.000 - 0s 70us/step - loss: 0.0391 - acc: 0.9838 - val_loss: 0.0460 - val_acc: 0.9758\n",
      "Epoch 318/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0705 - acc: 0.968 - 0s 73us/step - loss: 0.0461 - acc: 0.9811 - val_loss: 0.0365 - val_acc: 0.9839\n",
      "Epoch 319/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0161 - acc: 1.000 - 0s 70us/step - loss: 0.0570 - acc: 0.9757 - val_loss: 0.0424 - val_acc: 0.9758\n",
      "Epoch 320/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0455 - acc: 1.000 - 0s 70us/step - loss: 0.0506 - acc: 0.9757 - val_loss: 0.0511 - val_acc: 0.9839\n",
      "Epoch 321/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0997 - acc: 0.968 - 0s 70us/step - loss: 0.0687 - acc: 0.9650 - val_loss: 0.1207 - val_acc: 0.9516\n",
      "Epoch 322/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0428 - acc: 1.000 - 0s 67us/step - loss: 0.0850 - acc: 0.9596 - val_loss: 0.0508 - val_acc: 0.9839\n",
      "Epoch 323/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0428 - acc: 0.968 - 0s 67us/step - loss: 0.0625 - acc: 0.9730 - val_loss: 0.0660 - val_acc: 0.9758\n",
      "Epoch 324/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0140 - acc: 1.000 - 0s 70us/step - loss: 0.0442 - acc: 0.9838 - val_loss: 0.0580 - val_acc: 0.9597\n",
      "Epoch 325/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0725 - acc: 0.937 - 0s 67us/step - loss: 0.0496 - acc: 0.9784 - val_loss: 0.0502 - val_acc: 0.9677\n",
      "Epoch 326/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0430 - acc: 0.968 - 0s 70us/step - loss: 0.0424 - acc: 0.9784 - val_loss: 0.0609 - val_acc: 0.9839\n",
      "Epoch 327/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0937 - acc: 0.937 - 0s 67us/step - loss: 0.0570 - acc: 0.9730 - val_loss: 0.0562 - val_acc: 0.9839\n",
      "Epoch 328/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0149 - acc: 1.000 - 0s 67us/step - loss: 0.0621 - acc: 0.9757 - val_loss: 0.0787 - val_acc: 0.9597\n",
      "Epoch 329/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1994 - acc: 0.937 - 0s 67us/step - loss: 0.0623 - acc: 0.9757 - val_loss: 0.0568 - val_acc: 0.9839\n",
      "Epoch 330/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371/371 [==============================] - ETA: 0s - loss: 0.0216 - acc: 1.000 - 0s 67us/step - loss: 0.0690 - acc: 0.9704 - val_loss: 0.0478 - val_acc: 0.9677\n",
      "Epoch 331/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1282 - acc: 0.968 - 0s 65us/step - loss: 0.0521 - acc: 0.9811 - val_loss: 0.0656 - val_acc: 0.9758\n",
      "Epoch 332/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0177 - acc: 1.000 - 0s 73us/step - loss: 0.0409 - acc: 0.9838 - val_loss: 0.0600 - val_acc: 0.9839\n",
      "Epoch 333/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0747 - acc: 0.968 - 0s 67us/step - loss: 0.0411 - acc: 0.9811 - val_loss: 0.0576 - val_acc: 0.9758\n",
      "Epoch 334/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0223 - acc: 1.000 - 0s 67us/step - loss: 0.0447 - acc: 0.9811 - val_loss: 0.0426 - val_acc: 0.9839\n",
      "Epoch 335/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 6.5508e-04 - acc: 1.000 - 0s 67us/step - loss: 0.0457 - acc: 0.9784 - val_loss: 0.0647 - val_acc: 0.9758\n",
      "Epoch 336/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0719 - acc: 0.968 - 0s 67us/step - loss: 0.0397 - acc: 0.9865 - val_loss: 0.0408 - val_acc: 0.9839\n",
      "Epoch 337/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0612 - acc: 0.968 - 0s 69us/step - loss: 0.0394 - acc: 0.9838 - val_loss: 0.0545 - val_acc: 0.9677\n",
      "Epoch 338/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0082 - acc: 1.000 - 0s 62us/step - loss: 0.0362 - acc: 0.9811 - val_loss: 0.0866 - val_acc: 0.9677\n",
      "Epoch 339/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0147 - acc: 1.000 - 0s 67us/step - loss: 0.0313 - acc: 0.9946 - val_loss: 0.0449 - val_acc: 0.9839\n",
      "Epoch 340/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0639 - acc: 0.968 - 0s 67us/step - loss: 0.0467 - acc: 0.9811 - val_loss: 0.0315 - val_acc: 0.9919\n",
      "Epoch 341/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0201 - acc: 1.000 - 0s 67us/step - loss: 0.0409 - acc: 0.9811 - val_loss: 0.0522 - val_acc: 0.9677\n",
      "Epoch 342/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0021 - acc: 1.000 - 0s 67us/step - loss: 0.0430 - acc: 0.9784 - val_loss: 0.0622 - val_acc: 0.9758\n",
      "Epoch 343/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0070 - acc: 1.000 - 0s 65us/step - loss: 0.0455 - acc: 0.9784 - val_loss: 0.0603 - val_acc: 0.9677\n",
      "Epoch 344/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1027 - acc: 0.937 - 0s 67us/step - loss: 0.0490 - acc: 0.9757 - val_loss: 0.0545 - val_acc: 0.9839\n",
      "Epoch 345/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0203 - acc: 1.000 - 0s 67us/step - loss: 0.0703 - acc: 0.9730 - val_loss: 0.1023 - val_acc: 0.9355\n",
      "Epoch 346/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0322 - acc: 1.000 - 0s 66us/step - loss: 0.0413 - acc: 0.9811 - val_loss: 0.0721 - val_acc: 0.9597\n",
      "Epoch 347/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0234 - acc: 1.000 - 0s 70us/step - loss: 0.0528 - acc: 0.9784 - val_loss: 0.0715 - val_acc: 0.9758\n",
      "Epoch 348/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0225 - acc: 1.000 - 0s 67us/step - loss: 0.0651 - acc: 0.9757 - val_loss: 0.0535 - val_acc: 0.9677\n",
      "Epoch 349/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0112 - acc: 1.000 - 0s 65us/step - loss: 0.0643 - acc: 0.9704 - val_loss: 0.0912 - val_acc: 0.9516\n",
      "Epoch 350/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0959 - acc: 0.968 - 0s 67us/step - loss: 0.0838 - acc: 0.9596 - val_loss: 0.0788 - val_acc: 0.9597\n",
      "Epoch 351/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1466 - acc: 0.968 - 0s 70us/step - loss: 0.0763 - acc: 0.9784 - val_loss: 0.0677 - val_acc: 0.9677\n",
      "Epoch 352/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0177 - acc: 1.000 - 0s 73us/step - loss: 0.0787 - acc: 0.9730 - val_loss: 0.1327 - val_acc: 0.9355\n",
      "Epoch 353/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0601 - acc: 0.968 - 0s 70us/step - loss: 0.0975 - acc: 0.9704 - val_loss: 0.0618 - val_acc: 0.9839\n",
      "Epoch 354/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0459 - acc: 0.968 - 0s 67us/step - loss: 0.0676 - acc: 0.9784 - val_loss: 0.0677 - val_acc: 0.9597\n",
      "Epoch 355/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0096 - acc: 1.000 - 0s 67us/step - loss: 0.0794 - acc: 0.9650 - val_loss: 0.0458 - val_acc: 0.9758\n",
      "Epoch 356/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1029 - acc: 0.968 - 0s 70us/step - loss: 0.0445 - acc: 0.9784 - val_loss: 0.0425 - val_acc: 0.9758\n",
      "Epoch 357/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0710 - acc: 0.968 - 0s 70us/step - loss: 0.0570 - acc: 0.9704 - val_loss: 0.0578 - val_acc: 0.9677\n",
      "Epoch 358/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0414 - acc: 0.968 - 0s 66us/step - loss: 0.0406 - acc: 0.9811 - val_loss: 0.0755 - val_acc: 0.9677\n",
      "Epoch 359/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0275 - acc: 0.968 - 0s 73us/step - loss: 0.0361 - acc: 0.9865 - val_loss: 0.0468 - val_acc: 0.9839\n",
      "Epoch 360/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0156 - acc: 1.000 - 0s 70us/step - loss: 0.0514 - acc: 0.9784 - val_loss: 0.0958 - val_acc: 0.9435\n",
      "Epoch 361/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0123 - acc: 1.000 - 0s 67us/step - loss: 0.0349 - acc: 0.9865 - val_loss: 0.0408 - val_acc: 0.9758\n",
      "Epoch 362/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0030 - acc: 1.000 - 0s 67us/step - loss: 0.0441 - acc: 0.9811 - val_loss: 0.0401 - val_acc: 0.9677\n",
      "Epoch 363/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0063 - acc: 1.000 - 0s 73us/step - loss: 0.0251 - acc: 0.9946 - val_loss: 0.0466 - val_acc: 0.9758\n",
      "Epoch 364/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0321 - acc: 1.000 - 0s 73us/step - loss: 0.0333 - acc: 0.9865 - val_loss: 0.0674 - val_acc: 0.9597\n",
      "Epoch 365/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0220 - acc: 1.000 - 0s 67us/step - loss: 0.0398 - acc: 0.9811 - val_loss: 0.0517 - val_acc: 0.9597\n",
      "Epoch 366/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0036 - acc: 1.000 - 0s 65us/step - loss: 0.0466 - acc: 0.9784 - val_loss: 0.0706 - val_acc: 0.9516\n",
      "Epoch 367/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0922 - acc: 0.968 - 0s 73us/step - loss: 0.0343 - acc: 0.9865 - val_loss: 0.0758 - val_acc: 0.9758\n",
      "Epoch 368/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0537 - acc: 0.937 - 0s 70us/step - loss: 0.0444 - acc: 0.9784 - val_loss: 0.0476 - val_acc: 0.9839\n",
      "Epoch 369/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0090 - acc: 1.000 - 0s 70us/step - loss: 0.0281 - acc: 0.9892 - val_loss: 0.0387 - val_acc: 0.9919\n",
      "Epoch 370/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0182 - acc: 1.000 - 0s 73us/step - loss: 0.0253 - acc: 0.9865 - val_loss: 0.0745 - val_acc: 0.9516\n",
      "Epoch 371/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0113 - acc: 1.000 - 0s 69us/step - loss: 0.0268 - acc: 0.9919 - val_loss: 0.0458 - val_acc: 0.9839\n",
      "Epoch 372/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0221 - acc: 1.000 - 0s 67us/step - loss: 0.0289 - acc: 0.9892 - val_loss: 0.0897 - val_acc: 0.9597\n",
      "Epoch 373/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0692 - acc: 0.937 - 0s 65us/step - loss: 0.0472 - acc: 0.9784 - val_loss: 0.0632 - val_acc: 0.9597\n",
      "Epoch 374/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0345 - acc: 0.968 - 0s 67us/step - loss: 0.1193 - acc: 0.9677 - val_loss: 0.0728 - val_acc: 0.9839\n",
      "Epoch 375/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0748 - acc: 0.968 - 0s 67us/step - loss: 0.0669 - acc: 0.9650 - val_loss: 0.0700 - val_acc: 0.9597\n",
      "Epoch 376/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0790 - acc: 0.968 - 0s 75us/step - loss: 0.0502 - acc: 0.9838 - val_loss: 0.0412 - val_acc: 0.9758\n",
      "Epoch 377/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371/371 [==============================] - ETA: 0s - loss: 0.0172 - acc: 1.000 - 0s 67us/step - loss: 0.0733 - acc: 0.9811 - val_loss: 0.0498 - val_acc: 0.9758\n",
      "Epoch 378/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1418 - acc: 0.968 - 0s 65us/step - loss: 0.0713 - acc: 0.9704 - val_loss: 0.0513 - val_acc: 0.9919\n",
      "Epoch 379/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0345 - acc: 0.968 - 0s 70us/step - loss: 0.0963 - acc: 0.9677 - val_loss: 0.0762 - val_acc: 0.9677\n",
      "Epoch 380/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0528 - acc: 0.968 - 0s 67us/step - loss: 0.0662 - acc: 0.9730 - val_loss: 0.0495 - val_acc: 0.9758\n",
      "Epoch 381/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0964 - acc: 0.937 - 0s 65us/step - loss: 0.0457 - acc: 0.9730 - val_loss: 0.0837 - val_acc: 0.9839\n",
      "Epoch 382/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0020 - acc: 1.000 - 0s 75us/step - loss: 0.0540 - acc: 0.9784 - val_loss: 0.0564 - val_acc: 0.9677\n",
      "Epoch 383/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0605 - acc: 0.968 - 0s 70us/step - loss: 0.0379 - acc: 0.9811 - val_loss: 0.0502 - val_acc: 0.9677\n",
      "Epoch 384/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0153 - acc: 1.000 - 0s 67us/step - loss: 0.0408 - acc: 0.9838 - val_loss: 0.0584 - val_acc: 0.9677\n",
      "Epoch 385/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0634 - acc: 0.968 - 0s 67us/step - loss: 0.0723 - acc: 0.9730 - val_loss: 0.0542 - val_acc: 0.9597\n",
      "Epoch 386/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0046 - acc: 1.000 - 0s 67us/step - loss: 0.0486 - acc: 0.9865 - val_loss: 0.0524 - val_acc: 0.9677\n",
      "Epoch 387/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0224 - acc: 1.000 - 0s 70us/step - loss: 0.0367 - acc: 0.9892 - val_loss: 0.0454 - val_acc: 0.9677\n",
      "Epoch 388/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0188 - acc: 1.000 - 0s 67us/step - loss: 0.0293 - acc: 0.9946 - val_loss: 0.0586 - val_acc: 0.9597\n",
      "Epoch 389/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0331 - acc: 1.000 - 0s 70us/step - loss: 0.0604 - acc: 0.9757 - val_loss: 0.0570 - val_acc: 0.9677\n",
      "Epoch 390/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0246 - acc: 1.000 - 0s 70us/step - loss: 0.0457 - acc: 0.9838 - val_loss: 0.0714 - val_acc: 0.9597\n",
      "Epoch 391/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0114 - acc: 1.000 - 0s 67us/step - loss: 0.0646 - acc: 0.9784 - val_loss: 0.0748 - val_acc: 0.9677\n",
      "Epoch 392/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0934 - acc: 0.968 - 0s 70us/step - loss: 0.0512 - acc: 0.9784 - val_loss: 0.0534 - val_acc: 0.9597\n",
      "Epoch 393/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0129 - acc: 1.000 - 0s 70us/step - loss: 0.0393 - acc: 0.9811 - val_loss: 0.0472 - val_acc: 0.9677\n",
      "Epoch 394/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0326 - acc: 1.000 - 0s 70us/step - loss: 0.0400 - acc: 0.9811 - val_loss: 0.0632 - val_acc: 0.9597\n",
      "Epoch 395/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0146 - acc: 1.000 - 0s 67us/step - loss: 0.0442 - acc: 0.9757 - val_loss: 0.0553 - val_acc: 0.9839\n",
      "Epoch 396/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0420 - acc: 0.968 - 0s 70us/step - loss: 0.0866 - acc: 0.9650 - val_loss: 0.1463 - val_acc: 0.9516\n",
      "Epoch 397/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0827 - acc: 0.968 - 0s 67us/step - loss: 0.0707 - acc: 0.9757 - val_loss: 0.0570 - val_acc: 0.9758\n",
      "Epoch 398/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0965 - acc: 0.968 - 0s 67us/step - loss: 0.0451 - acc: 0.9811 - val_loss: 0.1236 - val_acc: 0.9516\n",
      "Epoch 399/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0327 - acc: 1.000 - 0s 67us/step - loss: 0.0501 - acc: 0.9865 - val_loss: 0.1040 - val_acc: 0.9516\n",
      "Epoch 400/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0585 - acc: 0.968 - 0s 73us/step - loss: 0.0528 - acc: 0.9811 - val_loss: 0.0754 - val_acc: 0.9758\n",
      "Epoch 401/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0440 - acc: 0.968 - 0s 70us/step - loss: 0.0553 - acc: 0.9704 - val_loss: 0.0859 - val_acc: 0.9597\n",
      "Epoch 402/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0168 - acc: 1.000 - 0s 73us/step - loss: 0.0526 - acc: 0.9730 - val_loss: 0.0547 - val_acc: 0.9597\n",
      "Epoch 403/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1987 - acc: 0.937 - 0s 78us/step - loss: 0.0630 - acc: 0.9730 - val_loss: 0.0715 - val_acc: 0.9597\n",
      "Epoch 404/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0817 - acc: 0.937 - 0s 70us/step - loss: 0.0540 - acc: 0.9704 - val_loss: 0.0484 - val_acc: 0.9677\n",
      "Epoch 405/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1495 - acc: 0.968 - 0s 67us/step - loss: 0.0626 - acc: 0.9704 - val_loss: 0.0459 - val_acc: 0.9839\n",
      "Epoch 406/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1341 - acc: 0.937 - 0s 75us/step - loss: 0.0539 - acc: 0.9811 - val_loss: 0.0886 - val_acc: 0.9677\n",
      "Epoch 407/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0487 - acc: 0.968 - 0s 67us/step - loss: 0.0432 - acc: 0.9838 - val_loss: 0.0849 - val_acc: 0.9597\n",
      "Epoch 408/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0327 - acc: 1.000 - 0s 70us/step - loss: 0.0451 - acc: 0.9838 - val_loss: 0.0727 - val_acc: 0.9758\n",
      "Epoch 409/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.2065 - acc: 0.875 - 0s 67us/step - loss: 0.0509 - acc: 0.9730 - val_loss: 0.0493 - val_acc: 0.9758\n",
      "Epoch 410/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0612 - acc: 0.968 - 0s 73us/step - loss: 0.0356 - acc: 0.9892 - val_loss: 0.0725 - val_acc: 0.9597\n",
      "Epoch 411/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0277 - acc: 1.000 - 0s 71us/step - loss: 0.0387 - acc: 0.9811 - val_loss: 0.0515 - val_acc: 0.9839\n",
      "Epoch 412/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0016 - acc: 1.000 - 0s 67us/step - loss: 0.0459 - acc: 0.9757 - val_loss: 0.0685 - val_acc: 0.9677\n",
      "Epoch 413/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0689 - acc: 0.968 - 0s 67us/step - loss: 0.0615 - acc: 0.9704 - val_loss: 0.0458 - val_acc: 0.9597\n",
      "Epoch 414/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1112 - acc: 0.937 - 0s 70us/step - loss: 0.0359 - acc: 0.9811 - val_loss: 0.0422 - val_acc: 0.9839\n",
      "Epoch 415/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0813 - acc: 0.968 - 0s 70us/step - loss: 0.0452 - acc: 0.9757 - val_loss: 0.0461 - val_acc: 0.9839\n",
      "Epoch 416/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0060 - acc: 1.000 - 0s 65us/step - loss: 0.0313 - acc: 0.9919 - val_loss: 0.0464 - val_acc: 0.9839\n",
      "Epoch 417/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0853 - acc: 0.968 - 0s 73us/step - loss: 0.0451 - acc: 0.9784 - val_loss: 0.0529 - val_acc: 0.9597\n",
      "Epoch 418/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0211 - acc: 1.000 - 0s 70us/step - loss: 0.0379 - acc: 0.9865 - val_loss: 0.0374 - val_acc: 0.9919\n",
      "Epoch 419/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0703 - acc: 0.937 - 0s 70us/step - loss: 0.0492 - acc: 0.9677 - val_loss: 0.0356 - val_acc: 0.9839\n",
      "Epoch 420/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0538 - acc: 0.968 - 0s 73us/step - loss: 0.0435 - acc: 0.9811 - val_loss: 0.0384 - val_acc: 0.9839\n",
      "Epoch 421/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0032 - acc: 1.000 - 0s 73us/step - loss: 0.0349 - acc: 0.9892 - val_loss: 0.0355 - val_acc: 0.9839\n",
      "Epoch 422/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0537 - acc: 0.968 - 0s 65us/step - loss: 0.0568 - acc: 0.9757 - val_loss: 0.0631 - val_acc: 0.9677\n",
      "Epoch 423/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0242 - acc: 1.000 - 0s 67us/step - loss: 0.0512 - acc: 0.9757 - val_loss: 0.0498 - val_acc: 0.9839\n",
      "Epoch 424/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371/371 [==============================] - ETA: 0s - loss: 0.0840 - acc: 0.968 - 0s 73us/step - loss: 0.0453 - acc: 0.9784 - val_loss: 0.1097 - val_acc: 0.9677\n",
      "Epoch 425/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0158 - acc: 1.000 - 0s 70us/step - loss: 0.0655 - acc: 0.9677 - val_loss: 0.0822 - val_acc: 0.9677\n",
      "Epoch 426/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0068 - acc: 1.000 - 0s 67us/step - loss: 0.0507 - acc: 0.9757 - val_loss: 0.0693 - val_acc: 0.9597\n",
      "Epoch 427/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0962 - acc: 0.937 - 0s 67us/step - loss: 0.0989 - acc: 0.9623 - val_loss: 0.1047 - val_acc: 0.9516\n",
      "Epoch 428/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0151 - acc: 1.000 - 0s 71us/step - loss: 0.0486 - acc: 0.9757 - val_loss: 0.0487 - val_acc: 0.9839\n",
      "Epoch 429/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0272 - acc: 0.968 - 0s 70us/step - loss: 0.0412 - acc: 0.9838 - val_loss: 0.1600 - val_acc: 0.9355\n",
      "Epoch 430/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0524 - acc: 1.000 - 0s 73us/step - loss: 0.0712 - acc: 0.9730 - val_loss: 0.0615 - val_acc: 0.9758\n",
      "Epoch 431/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0417 - acc: 0.968 - 0s 70us/step - loss: 0.0509 - acc: 0.9784 - val_loss: 0.0425 - val_acc: 0.9839\n",
      "Epoch 432/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0234 - acc: 1.000 - 0s 70us/step - loss: 0.0534 - acc: 0.9704 - val_loss: 0.0809 - val_acc: 0.9677\n",
      "Epoch 433/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0284 - acc: 1.000 - 0s 70us/step - loss: 0.0399 - acc: 0.9784 - val_loss: 0.0420 - val_acc: 0.9839\n",
      "Epoch 434/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0938 - acc: 0.937 - 0s 70us/step - loss: 0.0363 - acc: 0.9838 - val_loss: 0.1590 - val_acc: 0.9435\n",
      "Epoch 435/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0070 - acc: 1.000 - 0s 67us/step - loss: 0.0716 - acc: 0.9811 - val_loss: 0.0397 - val_acc: 0.9839\n",
      "Epoch 436/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0042 - acc: 1.000 - 0s 67us/step - loss: 0.0539 - acc: 0.9730 - val_loss: 0.0619 - val_acc: 0.9597\n",
      "Epoch 437/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0087 - acc: 1.000 - 0s 65us/step - loss: 0.0498 - acc: 0.9730 - val_loss: 0.1005 - val_acc: 0.9516\n",
      "Epoch 438/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0355 - acc: 0.968 - 0s 73us/step - loss: 0.0329 - acc: 0.9865 - val_loss: 0.0705 - val_acc: 0.9677\n",
      "Epoch 439/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0275 - acc: 0.968 - 0s 71us/step - loss: 0.0332 - acc: 0.9892 - val_loss: 0.0705 - val_acc: 0.9516\n",
      "Epoch 440/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0355 - acc: 1.000 - 0s 70us/step - loss: 0.0252 - acc: 0.9946 - val_loss: 0.0492 - val_acc: 0.9677\n",
      "Epoch 441/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0574 - acc: 0.968 - 0s 70us/step - loss: 0.0214 - acc: 0.9919 - val_loss: 0.0405 - val_acc: 0.9839\n",
      "Epoch 442/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0237 - acc: 1.000 - 0s 70us/step - loss: 0.0231 - acc: 0.9919 - val_loss: 0.0801 - val_acc: 0.9597\n",
      "Epoch 443/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0383 - acc: 1.000 - 0s 70us/step - loss: 0.0301 - acc: 0.9919 - val_loss: 0.0730 - val_acc: 0.9677\n",
      "Epoch 444/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0104 - acc: 1.000 - 0s 70us/step - loss: 0.0359 - acc: 0.9838 - val_loss: 0.0906 - val_acc: 0.9597\n",
      "Epoch 445/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0139 - acc: 1.000 - 0s 70us/step - loss: 0.0578 - acc: 0.9730 - val_loss: 0.0508 - val_acc: 0.9839\n",
      "Epoch 446/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1154 - acc: 0.937 - 0s 70us/step - loss: 0.0728 - acc: 0.9623 - val_loss: 0.0512 - val_acc: 0.9919\n",
      "Epoch 447/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0961 - acc: 0.968 - 0s 70us/step - loss: 0.1508 - acc: 0.9407 - val_loss: 0.0759 - val_acc: 0.9516\n",
      "Epoch 448/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0052 - acc: 1.000 - 0s 75us/step - loss: 0.0901 - acc: 0.9596 - val_loss: 0.0566 - val_acc: 0.9677\n",
      "Epoch 449/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0043 - acc: 1.000 - 0s 69us/step - loss: 0.0836 - acc: 0.9677 - val_loss: 0.0945 - val_acc: 0.9597\n",
      "Epoch 450/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1491 - acc: 0.937 - 0s 70us/step - loss: 0.0746 - acc: 0.9704 - val_loss: 0.0678 - val_acc: 0.9597\n",
      "Epoch 451/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1020 - acc: 0.968 - 0s 70us/step - loss: 0.0586 - acc: 0.9704 - val_loss: 0.0738 - val_acc: 0.9516\n",
      "Epoch 452/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0236 - acc: 1.000 - 0s 70us/step - loss: 0.0898 - acc: 0.9542 - val_loss: 0.0725 - val_acc: 0.9597\n",
      "Epoch 453/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0036 - acc: 1.000 - 0s 67us/step - loss: 0.0538 - acc: 0.9730 - val_loss: 0.0457 - val_acc: 0.9919\n",
      "Epoch 454/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0205 - acc: 1.000 - 0s 70us/step - loss: 0.0446 - acc: 0.9838 - val_loss: 0.0499 - val_acc: 0.9758\n",
      "Epoch 455/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0093 - acc: 1.000 - 0s 73us/step - loss: 0.0357 - acc: 0.9838 - val_loss: 0.0438 - val_acc: 0.9677\n",
      "Epoch 456/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0281 - acc: 1.000 - 0s 62us/step - loss: 0.0392 - acc: 0.9892 - val_loss: 0.0923 - val_acc: 0.9597\n",
      "Epoch 457/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0014 - acc: 1.000 - 0s 73us/step - loss: 0.0416 - acc: 0.9757 - val_loss: 0.0506 - val_acc: 0.9839\n",
      "Epoch 458/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0032 - acc: 1.000 - 0s 70us/step - loss: 0.0344 - acc: 0.9865 - val_loss: 0.0562 - val_acc: 0.9758\n",
      "Epoch 459/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0392 - acc: 0.968 - 0s 75us/step - loss: 0.0449 - acc: 0.9757 - val_loss: 0.0434 - val_acc: 0.9839\n",
      "Epoch 460/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0033 - acc: 1.000 - 0s 69us/step - loss: 0.0328 - acc: 0.9865 - val_loss: 0.0414 - val_acc: 0.9839\n",
      "Epoch 461/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0502 - acc: 0.968 - 0s 70us/step - loss: 0.0336 - acc: 0.9865 - val_loss: 0.0595 - val_acc: 0.9597\n",
      "Epoch 462/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0603 - acc: 0.968 - 0s 73us/step - loss: 0.0359 - acc: 0.9784 - val_loss: 0.0605 - val_acc: 0.9758\n",
      "Epoch 463/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0384 - acc: 0.968 - 0s 73us/step - loss: 0.0250 - acc: 0.9919 - val_loss: 0.0482 - val_acc: 0.9839\n",
      "Epoch 464/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0611 - acc: 0.968 - 0s 67us/step - loss: 0.0391 - acc: 0.9811 - val_loss: 0.0594 - val_acc: 0.9758\n",
      "Epoch 465/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0124 - acc: 1.000 - 0s 65us/step - loss: 0.0524 - acc: 0.9784 - val_loss: 0.0425 - val_acc: 0.9919\n",
      "Epoch 466/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0670 - acc: 0.968 - 0s 70us/step - loss: 0.0595 - acc: 0.9757 - val_loss: 0.0650 - val_acc: 0.9597\n",
      "Epoch 467/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0016 - acc: 1.000 - 0s 75us/step - loss: 0.0495 - acc: 0.9784 - val_loss: 0.0520 - val_acc: 0.9597\n",
      "Epoch 468/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0265 - acc: 1.000 - 0s 67us/step - loss: 0.0486 - acc: 0.9730 - val_loss: 0.0659 - val_acc: 0.9677\n",
      "Epoch 469/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0030 - acc: 1.000 - 0s 73us/step - loss: 0.0326 - acc: 0.9838 - val_loss: 0.0851 - val_acc: 0.9597\n",
      "Epoch 470/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0561 - acc: 1.000 - 0s 73us/step - loss: 0.0366 - acc: 0.9865 - val_loss: 0.0645 - val_acc: 0.9677\n",
      "Epoch 471/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371/371 [==============================] - ETA: 0s - loss: 0.0119 - acc: 1.000 - 0s 70us/step - loss: 0.0438 - acc: 0.9811 - val_loss: 0.0562 - val_acc: 0.9597\n",
      "Epoch 472/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.2435 - acc: 0.906 - 0s 65us/step - loss: 0.0525 - acc: 0.9730 - val_loss: 0.0504 - val_acc: 0.9839\n",
      "Epoch 473/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0977 - acc: 0.968 - 0s 73us/step - loss: 0.0402 - acc: 0.9892 - val_loss: 0.0780 - val_acc: 0.9677\n",
      "Epoch 474/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0182 - acc: 1.000 - 0s 67us/step - loss: 0.0467 - acc: 0.9838 - val_loss: 0.0684 - val_acc: 0.9597\n",
      "Epoch 475/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0183 - acc: 1.000 - 0s 70us/step - loss: 0.0406 - acc: 0.9784 - val_loss: 0.0371 - val_acc: 0.9919\n",
      "Epoch 476/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.3630 - acc: 0.937 - 0s 67us/step - loss: 0.0700 - acc: 0.9757 - val_loss: 0.0482 - val_acc: 0.9677\n",
      "Epoch 477/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0489 - acc: 0.968 - 0s 70us/step - loss: 0.0409 - acc: 0.9757 - val_loss: 0.0459 - val_acc: 0.9839\n",
      "Epoch 478/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0503 - acc: 0.968 - 0s 67us/step - loss: 0.0461 - acc: 0.9811 - val_loss: 0.0419 - val_acc: 0.9839\n",
      "Epoch 479/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0021 - acc: 1.000 - 0s 67us/step - loss: 0.0336 - acc: 0.9892 - val_loss: 0.0441 - val_acc: 0.9839\n",
      "Epoch 480/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0112 - acc: 1.000 - 0s 70us/step - loss: 0.0321 - acc: 0.9892 - val_loss: 0.0311 - val_acc: 0.9919\n",
      "Epoch 481/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0119 - acc: 1.000 - 0s 67us/step - loss: 0.0302 - acc: 0.9865 - val_loss: 0.0396 - val_acc: 0.9839\n",
      "Epoch 482/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0205 - acc: 1.000 - 0s 67us/step - loss: 0.0489 - acc: 0.9757 - val_loss: 0.0472 - val_acc: 0.9839\n",
      "Epoch 483/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0244 - acc: 1.000 - 0s 67us/step - loss: 0.0427 - acc: 0.9838 - val_loss: 0.0470 - val_acc: 0.9677\n",
      "Epoch 484/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0773 - acc: 0.937 - 0s 70us/step - loss: 0.0545 - acc: 0.9704 - val_loss: 0.0580 - val_acc: 0.9839\n",
      "Epoch 485/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0467 - acc: 0.968 - 0s 65us/step - loss: 0.0914 - acc: 0.9596 - val_loss: 0.0945 - val_acc: 0.9597\n",
      "Epoch 486/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0097 - acc: 1.000 - 0s 67us/step - loss: 0.0491 - acc: 0.9838 - val_loss: 0.0367 - val_acc: 0.9758\n",
      "Epoch 487/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0249 - acc: 1.000 - 0s 70us/step - loss: 0.0227 - acc: 0.9973 - val_loss: 0.0578 - val_acc: 0.9677\n",
      "Epoch 488/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0193 - acc: 1.000 - 0s 65us/step - loss: 0.0362 - acc: 0.9811 - val_loss: 0.0385 - val_acc: 0.9758\n",
      "Epoch 489/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1151 - acc: 0.937 - 0s 65us/step - loss: 0.0694 - acc: 0.9704 - val_loss: 0.0586 - val_acc: 0.9758\n",
      "Epoch 490/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0485 - acc: 0.968 - 0s 67us/step - loss: 0.0505 - acc: 0.9704 - val_loss: 0.0903 - val_acc: 0.9516\n",
      "Epoch 491/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0135 - acc: 1.000 - 0s 67us/step - loss: 0.0442 - acc: 0.9757 - val_loss: 0.0394 - val_acc: 0.9839\n",
      "Epoch 492/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0020 - acc: 1.000 - 0s 65us/step - loss: 0.0211 - acc: 0.9946 - val_loss: 0.0387 - val_acc: 0.9758\n",
      "Epoch 493/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0362 - acc: 0.968 - 0s 70us/step - loss: 0.0326 - acc: 0.9865 - val_loss: 0.0325 - val_acc: 0.9758\n",
      "Epoch 494/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0060 - acc: 1.000 - 0s 69us/step - loss: 0.0709 - acc: 0.9704 - val_loss: 0.1432 - val_acc: 0.9597\n",
      "Epoch 495/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0678 - acc: 0.968 - 0s 67us/step - loss: 0.0770 - acc: 0.9704 - val_loss: 0.0924 - val_acc: 0.9516\n",
      "Epoch 496/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0783 - acc: 0.968 - 0s 70us/step - loss: 0.0644 - acc: 0.9784 - val_loss: 0.0371 - val_acc: 0.9839\n",
      "Epoch 497/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1269 - acc: 0.937 - 0s 70us/step - loss: 0.0549 - acc: 0.9811 - val_loss: 0.0380 - val_acc: 0.9839\n",
      "Epoch 498/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0020 - acc: 1.000 - 0s 70us/step - loss: 0.0446 - acc: 0.9784 - val_loss: 0.0754 - val_acc: 0.9597\n",
      "Epoch 499/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0459 - acc: 0.968 - 0s 65us/step - loss: 0.0452 - acc: 0.9757 - val_loss: 0.0637 - val_acc: 0.9839\n",
      "Epoch 500/500\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.0112 - acc: 1.000 - 0s 70us/step - loss: 0.0567 - acc: 0.9730 - val_loss: 0.0750 - val_acc: 0.9839\n"
     ]
    }
   ],
   "source": [
    "results = model.fit(X_train, y_train, epochs = 500, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, val in enumerate(y_pred):\n",
    "    if 1 - val < 0.5:\n",
    "        y_pred[idx] = 1\n",
    "    else:\n",
    "        y_pred[idx] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': '0.9838709677419355', 'precision': '0.9743589743589743', 'recall': '0.9743589743589743', 'f1': '0.9743589743589743'}\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d3c55b1320>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results.history[\"val_acc\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
